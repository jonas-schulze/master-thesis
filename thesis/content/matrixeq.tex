\chapter{Large-Scale Matrix Equations}

\section{Sparse Matrices}
Describe data structures to store sparse matrices.

\begin{example}[Non-zeros of Steel Profile]
  The Steel Profile of the Oberwolfach Benchmark Collection of size $n=371$
  contains of sparse matrices having
  $\nnz(E) = 2343$ or \SI{1.7}{\percent},
  $\nnz(A) = 2341$ or \SI{1.7}{\percent}, and
  $\nnz(B) = 87$ or \SI{3.4}{\percent} non-zero entries.
  Although not being stored sparsely, $\nnz(C) = 17$ or \SI{0.8}{\percent}.
\end{example}

Even if the system matrices $E, A, B, C$ are sparse,
the solution $X$ is dense.
However, it usually is of low numerical rank.

\section{Low-Rank Representations}

\todo{Find reference. Max Behr had a slide on that in his defense.}
\begin{theorem}[Low-Rank Solutions]
\label{thm:lowrank}
The solution $X$ to a \ac{DRE} is of low numerical rank.
\end{theorem}

Briefly describe classical Cholesky-type factorization, $X=ZZ^\T$.
This does only exist for symmetric positive-definite matrices $X$.

\todo[inline]{Decide on how to call the low-rank factors: $ZZ^\T$ vs $QQ^\T$.}
\todo[inline]{Decide on how to call the low-rank factors: $LDL^\T$ vs $QSQ^\T$.}

$Z$ and $L \in \R^{n\times r}$ are not necessarily triangular,
though there exist equivalent triangular decompositions.

\begin{proposition}[Triangular Low-Rank Factorizations]
  Let $X=ZZ^\T$ or $X=ZDZ^\T$ for some $Z\in\R^{n\times r}$ and $D\in\R^{r\times r}$,
  $r \leq n$,
  then there exist a lower triangular $\hat{Z}$
  and some $\hat D$ of the same size satisfying
  $X=\hat Z \hat Z^\T$
  or $X=\hat Z \hat D \hat Z^\T$.
\end{proposition}
\begin{proof}
  We are looking for a transformation $H\in\R^{r\times r}$ such that $\hat Z := ZH$ is triangular and $HH^\T = I_r$.
  Then $\hat Z := ZH$ and $\hat D := H^\T DH$ have the desired properties.

  \todo{How to handle row permutations?}
  Without loss of generality, let the first $r$ rows of $Z$ have full rank.
  Let $z_1\in\R^r$ denote the first row of $Z$.
  Let $H_r\in\R^{r\times r}$ be the \Householder transformation mapping
  $z_1$ onto $\norm{z_1}e_1 = (\norm{z_1},0,\ldots) \in \R^r$, \ie
  \begin{equation}
    H_r := I_r - \frac{2}{v^\T v} vv^T
  \end{equation}
  for $v := z_1 - \norm{z_1}e_1$.
  Note that $H_r H_r^\T = I_r$ and $H_r = H_r^\T$.
  As $Z$ has full rank, $v \neq 0$ and $H_r$ is well defined.
  Indeed,
  \begin{equation*}
    H_r z_1
    = z_1 - \frac{2v^\T z_1}{v^\T v} \cdot v
    = z_1 - 1 \cdot v
    = \norm{z_1}e_1
  \end{equation*}
  such that
  \begin{equation}
    Z H_r = (H_r Z^\T) = \begin{pmatrix}
      \norm{z_1} & 0 \\
      Z_{12} & Z_{22}
    \end{pmatrix}
  \end{equation}
  for some $Z_{12} \in \R^{n-1 \times 1}$ and $Z_{22} \in \R^{n-1 \times r-1}$.

  Similarly, let $z_2\in\R^{r-1}$ denote the first row of $Z_{22}$ and
  let $H_{r-1} \in \R^{r-1 \times r-1}$ be the \Householder matrix mapping
  $z_2$ onto $\norm{z_2}e_1 \in\R^{r-1}$.
  This iteratively defines the matrix
  \begin{equation}
    H :=
    H_r \cdot
    \begin{pmatrix}
      I_1 \\ & H_{r-1}
    \end{pmatrix}
    \cdots
    \begin{pmatrix}
      I_{r-2} \\ & H_2
    \end{pmatrix}
    .
  \end{equation}

  Note that $H_1 = I_1$ and
  that every factor in the product above is itself a \Householder transformation acting on $\R^r$.
  The corresponding vector has its first components filled with zeros.
\end{proof}

\subsection{Low-Rank Update Formula}
\label{sec:lr:update}

Let $A, B \in\R^{n\times r}$ and $C, D\in\R^{r\times r}$.
While an update for the \ac{LRCF} is straight-forward,
\begin{equation}
  AA^\T + BB^\T =
  \begin{bmatrix}
    A & B
  \end{bmatrix}
  \begin{bmatrix}
    A^\T \\ B^\T
  \end{bmatrix}
  ,
\end{equation}
a downdate is more involved.
One option is to use complex arithmetic, \ie
\begin{equation}
  AA^\T - BB^\T =
  \begin{bmatrix}
    A & \im B
  \end{bmatrix}
  \begin{bmatrix}
    A^\T \\ \im B^\T
  \end{bmatrix}
\end{equation}
where $\im^2=-1$.
This has several drawbacks.
The mathematical one is that we expect a real-valued\todo{Do we?}\ solution to the \ac{DRE}.
Hence, the imaginary part should converge to zero quickly,
or there should be an equivalent real-valued factorization.
This leads to two technical drawbacks.
Firstly, the complex arithmetic should not be necessary everywhere,
switching back to real arithmetic is highly non-trivial.\footnote{Don't be clever / KISS!}
\todo{Find reference!}
Secondly, complex arithmetic is inefficient, \ie has a more than the to-be-expected 2x penalty compared to real arithmetic.

Another option is to handle the parts separately which are added versus subtracted in the superposition,
and compute the difference afterwards.
While this may only require updates,
the overall result suffers from numerical cancellation
\cite[50]{Lang2015}
\cite[\pno~186, thesis~10]{Lang2017}.

A different strategy to represent matrices of low numerical rank is the \ac{LRSIF} \cite{Benner2009,Lang2015},
which does not require up- and downdate to be handled separately:
\begin{equation}
  ACA^\T \pm BDB^\T =
  \begin{bmatrix}
    A & B
  \end{bmatrix}
  \begin{bmatrix}
    C \\ & \pm D
  \end{bmatrix}
  \begin{bmatrix}
    A^\T \\ B^\T
  \end{bmatrix}
\end{equation}
The resulting low-rank object $QSQ^\T$ consists of much larger matrices,
$Q \in\R^{n\times 2r}$ and $S\in\R^{2r\times 2r}$.
Having \autoref{thm:lowrank} in mind,
and with iterative algorithms in sight,
the inner dimension must not grow arbitrarily.

\subsection{Column Compression}
\label{sec:lr:compression}

maximum-rank truncation,
singular value tolerance,
...

\todo{use proper algorithm}
Taken from \cite[Section 6.3.3]{Lang2017}:
Given $GSG^\T$, $G\in\Rnk$, and a tolerance $\epsilon\in\R$,
compute $G_rS_rG_r^\T \approx GSG^\T$, $G_r\in\Rnr$.
\begin{enumerate}
  \item
    Compute $G=QR\Pi^\T$ with $Q\in\Rnk$, $R\in\Rkk$ and a permutation $\Pi\in\Rkk$.
  \item
    Compute a decomposition $R \Pi^\T S \Pi R^\T = V \Lambda V^\T$ with $V \in \Rkk$
    and a diagonal matrix $\Lambda\in\Rkk$ with diagonal entries $\abs{\lambda_1}\geq\ldots\geq\abs{\lambda_k}$.
  \item
    Set $G_r := QV_r \in\Rnr$ and $S := \Lambda_r$ with $r\leq k$ and $\abs{\lambda_{r+1}}\leq\epsilon$.
    $V_r$ consists of the first $r$ columns of $V$ and $\Lambda_r$ of the first diagonal block of $\Lambda$.
\end{enumerate}

\todo[inline]{%
The algorithm above uses an absolute cut-off $\epsilon$.
How to integrate $G,S$ into a relative cut-off?
Maybe a \enquote{cheap} adaptation of \cite[Algorithm~2.2, line~2]{Lang2017},
or the even simpler $\epsilon\abs{\lambda_1}$?
}

\section{Model Order Reduction}
Give brief overview.
Details out of scope.
Assume problem to be given in reduced form;
solving the reduced order model then requires same algorithms/techniques.

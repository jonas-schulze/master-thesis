\chapter{Alternating-Directions Implicit Method}

The \ac{ADI} method goes back to \cite{Peaceman1955}.
It has originally been designed to solve linear systems
$Ax=b$
for symmetric positive-definite $A\in\Rnn$
and applied to elliptic and parabolic \acp{PDE} using a finite-difference scheme.
In this chapter, after stating the classical parametrized intermediate-/two-step formulation,
we describe its properties as a one-step method.
Next, we will apply the method to \ac{ALE} in \autoref{sec:adi:ale},
and determine suitable parameters in \autoref{sec:adi:parameters}.
Finally, we will mention some alternative solvers for \ac{ALE}.

Suppose $A=A_1+A_2$ for commuting $A_1, A_2$ and $x\in\R^n$ is a solution to $Ax=b$.
Then it is easy to see that
\begin{equation}
\label{eq:adi:shorthand}
\begin{aligned}
  (A_1 + \alpha I_n)x &= b - (A_2 - \alpha I_n) x \\
  (A_2 + \beta I_n)x &= b - (A_1 - \beta I_n) x
\end{aligned}
\end{equation}
for any $\alpha, \beta \in\C$.
Define the short-hand notation
\begin{align*}
  \Aip &:= A_1 + \alpha_k I_n & \Aiip &:= A_2 + \beta_k I_n \\
  \Aim &:= A_1 - \beta_k I_n  & \Aiim &:= A_2 - \alpha_k I_n
\end{align*}
and convert the above into an iteration scheme
\begin{equation}
  \label{eq:adi:general2step}
  \left\{
  \begin{aligned}
    \Aip  x^{k+\frac{1}{2}} &= b - \Aiim x^k \\
    \Aiip x^{k+1}           &= b - \Aim x^{k+\frac{1}{2}}
  \end{aligned}
  \right.
\end{equation}
where the parameters $\alpha_k, \beta_k \in\C$ are free to choose for every iteration $k\in\N$.

\section{ADI as a One-Step Method}
\label{sec:adi:1step}

Obviously, a solution to $Ax=b$ is a fixpoint of this iteration, $x = x^{k+1} = x^{k+\frac{1}{2}} = x^k$.
Conversely, a fixpoint $\hat x := x^{k+1} = x^k$ of the above is a solution to $Ax=b$,
provided that $\alpha_k + \beta_k$ is invertible.
Therefore, formulate the \ac{ADI}~\eqref{eq:adi:general2step} as a one-step scheme.

First, note that all the matrices $\Aip, \Aim, \Aiip, \Aiim$ commute as $A_1A_2 = A_2A_1$.
Thus, substituting the intermediate step $x^{k+\frac{1}{2}}$ into the final part of \eqref{eq:adi:general2step} leads to
\begin{align*}
  \Aiip \hat x
  &= b - \Aim \mathop{(\Aip)^{-1}} (b - \Aiim \hat x) \\
  &= b - \mathop{(\Aip)^{-1}} \Aim (b - \Aiim \hat x)
\end{align*}
by the lemma below.
Multiplying by $\Aip$ and rearranging the terms then gives
\begin{equation*}
  \underbrace{%
  (\Aip \Aiip - \Aim \Aiim)
  }_{(\alpha_k + \beta_k) (A_1+A_2)}
  \hat x =
  \underbrace{%
  (\Aip - \Aim)
  }_{(\alpha_k + \beta_k) I_n}
  b
  .
\end{equation*}
Finally, multiplying by $(\alpha_k + \beta_k)^{-1}$ shows $A \hat x = b$.

\begin{lemma}[Commuting Matrices]
\label{thm:adi:commuting-matrices}
  Let $\alpha, \beta\in\R$ and let $A, B \in\Rnn$ commute.
  If $A$ is invertible, then $A^{-1}B = BA^{-1}$.
  If $(A+\alpha I_n)$ is invertible, then
  \begin{equation*}
    (A+\alpha I_n)^{-1} (B+\beta I_n)
    =
    (B+\beta I_n) (A+\alpha I_n)^{-1}
    .
  \end{equation*}
\end{lemma}
\begin{proof}
  Multiplying $AB=BA$ with $A^{-1}$ from either side yields $A^{-1}B=BA^{-1}$.
  Furthermore, $AB=BA$ implies
  $
    (A+\alpha I_n) (B+\beta I_n)
    =
    (B+\beta I_n) (A+\alpha I_n)
  $.
  Applying the first claim yields the desired equality.
\end{proof}

\todo[inline]{The \ac{ADI} method can be seen as an operator splitting scheme.}

More rigorously,
using the notation defined at the beginning of the chapter,
as a general one-step iteration the \ac{ADI} reads
\begin{equation*}
\begin{aligned}
  x^{k+1}
  &= \mathop{(\Aiip)^{-1}} \big( b - \Aim \mathop{(\Aip)^{-1}} (b - \Aiim x^k) \big) \\
  &= \mathop{(\Aiip)^{-1}} \underbrace{
    \Aim \mathop{(\Aip)^{-1}}
  }_{
    \mathop{(\Aip)^{-1}} \Aim
  }
  \Aiim x^k + \mathop{(\Aiip)^{-1}} \big( I_n - \underbrace{
    \Aim \mathop{(\Aip)^{-1}}
  }_{
    \mathop{(\Aip)^{-1}} \Aim
  }
  \big) b
  .
\end{aligned}
\end{equation*}

\begin{lemma}[Cayley Transformation]
\label{thm:adi:cayley}
  Let $A, M\in\Cnn$ and $\alpha, \beta \in\C$ with $-\alpha\notin\Lambda(A, M)$.
  The \emph{generalized Cayley transformation} of a matrix pair $(A,M)$ is defined by
  the rational matrix function
  \begin{equation*}
    \Cayley(A, M, \alpha, \beta) := (A+\alpha M)^{-1} (A-\beta M).
  \end{equation*}
  Then it holds
  \begin{equation*}
    \Cayley(A, M, \alpha, \beta)
    = I_n - (\alpha+\beta) (A+\alpha M)^{-1} M
    .
  \end{equation*}
  \todo[inline]{
    Furthermore, $\Lambda(A,M) \subset\C_- \implies \rho(\Cayley(A,M,\alpha,\conj\alpha)) < 1$,
    \cf \cite[Proposition~2.16]{Kuerschner2016}.
  }
\end{lemma}
\begin{remark}
  In the context of control theory,
  the last claim states that
  $\Cayley(A, M, \alpha, \conj\alpha)$ is d-stable if
  $(A, M)$ is c-stable.
\end{remark}
\begin{proof}
  Note $A-\beta M = (A+\alpha M) - (\alpha+\beta)M$
  and multiply by $(A+\alpha M)^{-1}$.
\end{proof}

Therefore,
$
  (\Aip)^{-1}\Aim
  = \Cayley(A_1, I_n, \alpha_k, \beta_k)
  = I_n - (\alpha_k+\beta_k)(\Aip)^{-1}
$,
such that the iteration matrices for $x^{k+1} = M x^k + N b$ are given by:
\begin{equation}
\label{eq:adi:general1step}
  \begin{aligned}
    M &= \mathop{(\Aiip)^{-1}} \mathop{(\Aip)^{-1}} \Aim \Aiim \\
    N &= (\alpha_k + \beta_k) \mathop{(\Aiip)^{-1}} \mathop{(\Aip)^{-1}}
  \end{aligned}
\end{equation}

\begin{lemma}[Composition of Cayley Transformations]
\label{thm:adi:cayley-compose}
  Let $\alpha\in\C$ and let $A_1, A_2 \in\Rnn$ commute.
  Then there exists $B\in\Cnn$,
  \begin{equation*}
    B := A_1A_2 + \abs{\alpha}^2 I_n - 2\im \Im(\alpha) A_1
  \end{equation*}
  where $\im^2 = -1$,
  such that the composition
  \begin{align*}
    \MoveEqLeft
    \Cayley(A_1, I_n, \alpha, \conj{\alpha}) \cdot
    \Cayley(A_2, I_n, \conj{\alpha}, \alpha)
    \\
    &= \Cayley\big( B, A_1 + A_2, \alpha, \conj\alpha \big) \\
    &= I_n - 2\Re(\alpha)(\Aip\Aiip)^{-1}(A_1+A_2)
  \end{align*}
  is a Cayley transformation.
\end{lemma}
\begin{proof}
  Observe that
  $\Cayley(A_1, I_n, \alpha, \beta) = \Aipinv\Aim$ and
  $\Cayley(A_2, I_n, \beta, \alpha) = \Aiipinv\Aiim$
  using the short-hand notation~\eqref{eq:adi:shorthand},
  such that by \autoref{thm:adi:commuting-matrices}
  \begin{equation*}
    \Cayley(A_1, I_n, \alpha, \conj{\alpha})
    \cdot
    \Cayley(A_2, I_n, \conj{\alpha}, \alpha)
    =
    (\Aip\Aiip)^{-1} \Aim\Aiim
    .
  \end{equation*}
  Furthermore:
  \begin{align*}
    \Aip\Aiip &= A_1A_2 + \alpha\beta I_n + (\beta A_1 + \alpha A_2) \\
    \Aim\Aiim &= A_1A_2 + \alpha\beta I_n - (\alpha A_1 + \beta A_2)
  \end{align*}
  Applying $\beta := \conj\alpha$ as well as $\alpha = \conj\alpha + 2\im\Im(\alpha)$ yields:
  \begin{align*}
    \Aip\Aiip &= \big(A_1A_2 + \abs{\alpha}^2 I_n - 2\im\Im(\alpha)A_1\big) + \alpha (A_1+A_2) \\
    \Aim\Aiim &= \big(A_1A_2 + \abs{\alpha}^2 I_n - 2\im\Im(\alpha)A_1\big) - \conj\alpha (A_1+A_2)
  \end{align*}
  The final equality follows directly from \autoref{thm:adi:cayley}.
\end{proof}

\todo[inline]{%
  Applying this together with \autoref{thm:adi:cayley-compose}
  to an \ac{ALE}, $L = L_L + L_R$, leads to
  $
    X^{k+1} = X^k - 2\Re(\alpha_k) A_+^{-1} (A X^k + X^k A^\T + W) A_+^{-\HT}
  $
  which looks pretty but doesn't seem to be beneficial.
  Is this worth mentioning?
}

We have already seen that a fixpoint of the iteration solves $Ax=b$.
Following the standard literature,
the iteration converges iff $\rho(M) < 1$.
By \autoref{thm:adi:cayley} ...

\begin{hypothesis}[Convergence of ADI]
\label{thm:adi:convergence}
\todo{Rephrase in terms of eigenvalues}
  If $A$ is stable, then $\rho(M) < 1$.
\end{hypothesis}

\todo[inline]{%
  The ADI has super-linear convergence if $A_1A_2 = A_2A_1$.
  If this is easy to show, do so.
}

\section{Application to \act{ALE}s}
\label{sec:adi:ale}

Consider the \ac{ALE}
\begin{equation*}
\label{eq:adi:ale}
  L(X) := AX + XA^\HT = -W
\end{equation*}
where $W = W^\T$.
The \Lyapunov operator $L = L_L + L_R$ naturally decomposes into
left-multiplication $L_L : X \mapsto AX$ and
right-multiplication $L_R : X \mapsto XA^\HT$.
Furthermore, $L_L$ and $L_R$ commute:
\begin{equation*}
  L_L \circ L_R = L_R \circ L_L = X \mapsto AXA^\HT
\end{equation*}
All the operators above are linear and act on $\Rnn$.
Apply the \ac{ADI}~\eqref{eq:adi:general2step} as a two-step method:
\begin{equation}
  \left\{
  \begin{aligned}
    (A + \alpha_k I_n) X_{k+\frac{1}{2}} &= -W - X_k (A^\HT - \alpha_k I_n) \\
    X_{k+\frac{1}{2}} (A^\HT + \beta I_n) &= -W - (A - \beta I_n) X_k
  \end{aligned}
  \right.
\end{equation}

\begin{remark}
  Let $\alpha\in\C\setminus\R$ and $A_+ := A+\alpha I_n \in\Cnn$.
  As a mapping of matrices,
  $L_R + \alpha I_{n^2} : \Cnn \to \Cnn$ maps $U$ onto
  $
    U A^\HT + \alpha U =
    U(A+\conj{\alpha} I_n)^\HT \neq
    U A_+^\HT
  $.
  Therefore, the notation is slightly more involved than in the previous section.

  In the notation of the previous section,
  vectorize $\Cnn$ as $\C^{n^2}$ and identify the mappings
  \begin{gather*}
    A_1^\pm = L_L \pm \alpha_k I_{n^2}
    \qquad\text{with}\qquad
    A \pm \alpha_k I_n \in\Cnn \\
    A_2^\pm = L_R \pm \alpha_k I_{n^2}
    \qquad\text{with}\qquad
    A \pm \conj{\alpha_k} I_n \in\Cnn
  \end{gather*}
  together with left- and right-multiplication, respectively.
\end{remark}

The one-step formulation following \eqref{eq:adi:general1step} reads:
\begin{multline}
  X_{k+1} =
  (A + \alpha_k I_n)^{-1}
  (A - \beta_k I_n)
  X_k
  (A - \conj{\alpha_k} I_n)^\HT
  (A + \conj{\beta_k} I_n)^{-H}
  \\
  - (\alpha_k + \beta_k)
  (A + \alpha_k I_n)^{-1}
  W
  (A + \conj{\beta_k} I_n)^{-\HT}
\end{multline}
Choosing $\beta_k := \conj{\alpha_k}$
the above simplifies to
\begin{multline}
  X_{k+1} =
  (A + \alpha_k I_n)^{-1}
  (A - \conj{\alpha_k} I_n)
  X_k
  (A - \conj{\alpha_k} I_n)^\HT
  (A + \alpha_k I_n)^{-H}
  \\
  - 2\Re(\alpha_k)
  (A + \alpha_k I_n)^{-1}
  W
  (A + \alpha_k I_n)^{-\HT}
\end{multline}
which allows to apply a \ac{LRSIF}.
Taking $X_k = L_k D_k L_k^\T$ and $W = GSG^\T$
the \ac{ADI} becomes
\begin{align*}
  \hat L_{k+1} &= \begin{bmatrix}
    (A+\alpha_k I_n)^{-1} (A - \conj{\alpha_k} I_n) L_k &
    (A+\alpha_k I_n)^{-1} G
  \end{bmatrix} \\
  \hat D_{k+1} &= \begin{bmatrix}
    D_k \\
    & -2\Re(\alpha_k) S
  \end{bmatrix}
\end{align*}
and a compression $L_{k+1} D_{k+1} L_{k+1}^\T :\approx \hat L_{k+1} \hat D_{k+1} \hat L_{k+1}^\T$
following \autoref{sec:lr:compression}.
Taking the initial value $X_0 = 0$, the first iterate reads
\begin{align*}
  L_1 &= (A+\alpha_0 I_n)^{-1} G \\
  D_1 &= -2\Re(\alpha_0) S
  .
\end{align*}

\begin{remark}
  \citeauthor{Lang2017}~\cite{Lang2017} formulates the \ac{ALE}~\eqref{eq:adi:ale} in terms of the
  right-multiplication $\tilde L_R :X \mapsto XA^\T$, $\tilde L_R \neq L_R$.
  The ADI~\cite[Equation~(2.23)]{Lang2017} uses a somewhat inconsistent second step.
  This leads to the impression that the ADI is a one-parameter ADI, $\beta_k=\alpha_k$,
  instead of a true two-parameter ADI using $\beta_k = \conj{\alpha_k}$.
\end{remark}

\todo[inline]{%
Can $\rho(M) \leq \norm{M}$ (for any operator norm) be used to further simplify \cite[Algorithm~2.2, line~2]{Lang2017},
\ie can $\norm{}_2$ be replaced by \eg $\norm{}_F$?
As we are dealing with low-rank formulations of known rank $r$,
given that we ensure full rank of the factors,
we can \enquote{safely} exploit $\norm{A}_2 \leq \norm{A}_F \leq \sqrt{r}\norm{A}_2$.
}

\section{Parameter Selection}
\label{sec:adi:parameters}

After deriving the structure of the algorithm for \ac{ALE},
and the need of $\beta_k := \conj{\alpha_k}$ for a low-rank formulation,
we have to determine suitable parameters $\Set{\alpha_k : k\in\N}$.
Classical choices of pre-computed values include
optimal Wachspress shifts \cite{Wachspress1992,Wachspress2013} and
heuristic Penzl shifts \cite{Penzl1999}.
We resort to self-generating shifts described in \cite[Section~5.3]{Kuerschner2016}.

\section{Alternative Lyapunov Solvers}
Krylov subspace,
projection-based,
hybrid

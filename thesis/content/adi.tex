\chapter{Alternating-Directions Implicit Method}
\label{sec:ADI}

The \acf{ADI} method goes back to \citeauthor{Peaceman1955}~\cite{Peaceman1955}.
It has originally been designed to solve linear systems
$Ax=b$
for symmetric positive-definite $A\in\Rnn$
and applied to elliptic and parabolic \acp{PDE} using a finite-difference scheme.
This chapter first derives the method as a splitting method.
\autoref{sec:adi:ale} then applies the \ac{ADI} to an \ac{ALE} in \ac{LRSIF} following \citeauthor{Lang2017}~\cite{Lang2017}.
Finally, \autoref{sec:adi:parameters} describes how to choose the \ac{ADI} parameters following \citeauthor{Kuerschner2016}~\cite{Kuerschner2016}.

\section{Parametrized Splitting Schemes}

Many iterative methods solving $Ax=b$ can be written in a one-step \emph{splitting} form
\begin{equation}
  Mx^{k+1} = Nx^k + b
\end{equation}
where $M-N = A$ and systems $Mx = d$ are \enquote{easy} to solve \cite[Section~11.2.3]{Golub2013}.
These methods are \emph{consistent} by construction,
\ie every solution to $Ax=b$ is a fixpoint of the iteration.
Conversely, every fixpoint $x^{k+1} = x^k$ is a solution as well.
Throughout the chapter, assume that $M^{-1}$ exists.
The method converges if the spectral radius $\rho(G) < 1$ for $G:=M^{-1}N$ \cite[Theorem~11.2.1]{Golub2013}.
$G$ is called \emph{iteration matrix} of the scheme.

The structure above is called \emph{first normal form} of the method.
Consistency is equivalent to $M-N=A$.
The \emph{second normal form} of a consistent linear iteration method reads
\begin{equation}
  x^{k+1} = M^{-1} \big( (M-A) x^k + b \big) = x^k - M^{-1} (Ax^k - b)
\end{equation}
and $B := M^{-1}$ is called \emph{matrix of the second normal form}.

A splitting method is called \emph{parametrized} if the iteration matrix depends on the iteration,
\ie $A = M_k - N_k$ for $k\in\N$.
The normal forms are thus given by
\begin{subequations}
\label{eq:adi:normalform}
\begin{align}
\label{eq:adi:normalform:1}
  M_k x^{k+1} &= N_k x^k + b
  ,
  \\
\label{eq:adi:normalform:2}
  x^{k+1} &= x^k - M_k^{-1} (Ax^k - b)
  .
\end{align}
\end{subequations}
An operator split and a method is further called \emph{commuting} if $M_k, N_k$ commute, \ie
\begin{equation*}
  M_k N_k = N_k M_k
  \qquad
  \forall k \in\N,
\end{equation*}
and \emph{fully commuting} if $M_*, N_*$ commute, \ie
\begin{equation*}
  M_i N_j = N_j M_i
  \qquad
  M_i M_j = M_j M_i
  \qquad
  N_i N_j = N_j N_i
  \qquad
  \forall i,j \in\N
  .
\end{equation*}
In the latter case,
the order in which the steps defined by $(M_0,N_0)$ to $(M_k,N_k)$ are applied does not matter:

\begin{proposition}[Permutation Invariance]
\label{thm:adi:permutation}
  Let $A = M_k - N_k$ be a fully permuting operator split.
  Then, the value of $x^{k+1}$ given by the parametrized splitting method~\eqref{eq:adi:normalform}
  does not depend on the order of the steps.
\end{proposition}
\begin{proof}
  The following is a generalization of the proof of \citeauthor{Li2002}~\cite[Theorem~4.1]{Li2002}.
  Let $G_k := M_k^{-1} N_k$ denote the iteration matrix
  and $B_k := M_k^{-1}$ denote the matrix of the second normal form~\eqref{eq:adi:normalform:2}.
  Observe that the matrices $G_i, G_j$ as well as $A, B_i, B_j$ commute for $i,j\in\N$,
  since $ab=ba \implies a^{-1}b = ba^{-1}$ for any $a,b$ in a multiplicative group.
  Therefore, any neighboring steps may be exchanged,
  \begin{align*}
    x^{k+1}
    &= G_k x^k + B_k b \\
    &= G_k (G_{k-1} x^{k-1} + B_{k-1} b) + B_k b \\
    &= \underbrace{
      (G_k G_{k-1})
    }_{
      G_{k-1} G_k
    } x^{k-1} + \underbrace{
      (G_k B_{k-1} + B_k)
    }_{
      G_{k-1} B_k + B_{k-1}
    } b
  \end{align*}
  since $N_k = M_k - A$ implies $G_k = I_n - B_k A$ as well as
  \begin{align*}
    G_k B_{k-1} + B_k
    &= (I_n - B_k A) B_{k-1} + B_k \\
    &= B_{k-1} - \underbrace{
      B_k A B_{k-1}
    }_{
      B_{k-1} A B_k
    } + B_k \\
    &= B_{k-1} + (I_n - B_{k-1} A) B_k \\
    &= B_{k-1} + G_{k-1} B_k
    .
  \end{align*}
  As any permutation may be obtained by exchanging neighboring configurations,
  thus $x^k$ does not depend on the order of the steps.
\end{proof}

A well known fact is that the iteration matrix $G_k$ determines
the behavior of the \emph{error} $e^k := x^k - x^*$, where $Ax^* = b$.
Due to the consistency of the method, $M_k x^* = N_k x^* + b$.
Subtract this from the first normal form~\eqref{eq:adi:normalform:1} to obtain
$M_k e^{k+1} = N_k e^k$ or, equivalently,
\begin{equation}
\label{eq:adi:error-recursion}
  e^{k+1} = G_k e^k
  .
\end{equation}
An interesting observation is that for commuting operator splits,
the same recursion holds for the residual.
This leads to an alternative formulation of the second normal form.

\begin{proposition}[Recursive Residuals]
\label{thm:adi:residual}
  Let $A = M_k - N_k$ define a commuting parametrized splitting method~\eqref{eq:adi:normalform}.
  Then, the \emph{residual} $r^k := Ax^k - b$ adheres to
  \begin{equation*}
    r^{k+1} = G_k r^k
  \end{equation*}
  where $G_k := M_k^{-1} N_k$ denotes the iteration matrix.
\end{proposition}
\begin{proof}
  Due to $A = M_k - N_k$ it is $AM_k^{-1} = I - N_k M_k^{-1}$.
  Therefore,
  \begin{align*}
    r^{k+1}
    &= Ax^{k+1} - b \\
    &= A \big( M_k^{-1} (N_k x^k + b) \big) - b \\
    &= M_k^{-1} N_k \underbrace{
      \vphantom{M_k^{-1}}
      Ax^k
    }_{
      \vphantom{M_k^{-1}}
      r^k + b
    }
    + \underbrace{
      A M_k^{-1}
    }_{
      I\mathrlap{{} - N_k M_k^{-1}}
    } b - b \\
    &= M_k^{-1} N_k r^k + \underbrace{
      (M_k^{-1} N_k - N_k M_k^{-1})
    }_0 b
    .
  \end{align*}
\end{proof}

\begin{corollary}[Recursive Increment Form]
\label{thm:adi:increment-form}
  Let $A = M_k - N_k$ define a commuting parametrized splitting method~\eqref{eq:adi:normalform}.
  Then, the \emph{increment} $v^k := - M_k^{-1} r^k$ defining the second normal form~\eqref{eq:adi:normalform:2},
  \begin{equation*}
    x^{k+1} = x^k + v^k
  \end{equation*}
  adheres to
  \begin{equation*}
    M_{k+1} v^{k+1} = N_k v^k
  \end{equation*}
  where $r^k := Ax^k - b$ denotes the residual.
\end{corollary}

\begin{proof}
  By \autoref{thm:adi:residual} and $M_k N_k = N_k M_k$ it is
  \begin{equation*}
    M_{k+1} v^{k+1}
    = - r^{k+1}
    = - M_k^{-1} N_k r^k
    = N_k (- M_k^{-1} r^k)
    = N_k v^k
    .
  \end{equation*}
\end{proof}

\begin{remark}
  The above is a generalization of the \ac{ADI} reordering for \Lyapunov equations presented by \citeauthor{Li2002}~\cite[Section~4]{Li2002}.
  Their argument,
  which relies on \autoref{thm:adi:permutation},
  is outlined in \autoref{sec:li2002}.
  Note that \autoref{thm:adi:increment-form} does not require a fully commuting splitting method,
  nor does it restrict the initial value $x^0$.
\end{remark}

Reasoning about the convergence of parametrized splitting schemes is not as straight-forward as in the classical case.
To the best knowledge of the author,
only sufficient criteria and heuristics based on the following trivial theorem are known:

\begin{theorem}[Convergence of Splitting Schemes]
\label{thm:adi:convergence}
  Let $A = M_k - N_k$ define a parametrized splitting method~\eqref{eq:adi:normalform}.
  The method converges to $x = A^{-1} b$ for all starting points $x^0$
  if and only if $G_k \cdots G_0 \to 0$ where $G_k := M_k^{-1} N_k$.
\end{theorem}
\begin{proof}
  Follows directly from equation~\eqref{eq:adi:error-recursion}.
\end{proof}

\section{ADI as a Parametrized Splitting Scheme}
\label{sec:adi:1step}

Suppose $A=H+V$ for $A, H, V \in \Rnn$,
and $x\in\R^n$ is a solution to $Ax=b$.
Then it is easy to see that
\begin{equation}
\begin{aligned}
  (H + \alpha I_n)x &= b - (V - \alpha I_n) x \\
  (V + \beta I_n)x &= b - (H - \beta I_n) x
\end{aligned}
\end{equation}
for any $\alpha, \beta \in\C$.
Define the short-hand notation
\begin{equation}
\label{eq:adi:shorthand}
\begin{aligned}
  \Aip  &:= H + \alpha_k I_n &
  \qquad\qquad\qquad %FIXME
  \Aiip &:= V + \beta_k  I_n \\
  \Aim  &:= H - \beta_k  I_n &
  \Aiim &:= V - \alpha_k I_n
\end{aligned}
\end{equation}
and convert the above into an iteration scheme
\begin{equation}
  \label{eq:adi:general2step}
  \left\{
  \begin{aligned}
    \Aip  x^{k+\frac{1}{2}} &= b - \Aiim x^k \\
    \Aiip x^{k+1}           &= b - \Aim x^{k+\frac{1}{2}}
    ,
  \end{aligned}
  \right.
\end{equation}
where the parameters $\alpha_k, \beta_k \in\C$ are free to choose for every iteration $k\in\N$.

Obviously, every sub-step is a splitting scheme.
In fact, the \ac{ADI} as a whole can be seen as a parametrized splitting scheme.
Substituting the intermediate step $x^{k+\frac{1}{2}}$ into the full step $x^{k+1}$ leads to
\begin{equation}
  \Aiip x^{k+1}
  = b - \underbrace{
    \Aim\Aipinv
  }_{\Aipinv\Aim}
  (b - \Aiim \hat x^k)
  .
\end{equation}
Multiplying by $\Aip$ then gives
\begin{equation}
  \Aip\Aiip x^{k+1} = \Aim\Aiim x^k +
  \underbrace{%
  (\Aip - \Aim)
  }_{(\alpha_k + \beta_k) I_n}
  b
  .
\end{equation}
Finally, multiplying by $(\alpha_k + \beta_k)^{-1}$ reveals the splitting formulation in its first normal form
\begin{equation}
\label{eq:adi:general1step}
  \underbrace{
    (\alpha_k + \beta_k)^{-1} \Aip\Aiip \\
  }_{M_k}
  x^{k+1} =
  \underbrace{
    (\alpha_k + \beta_k)^{-1} \Aim\Aiim
  }_{N_k}
  x^k + b
  .
\end{equation}
Indeed, the splitting is consistent, $M_k - N_k = A$, since
\begin{equation}
\begin{aligned}
  \Aip\Aiip &= HV + \alpha_k\beta_k I_n + \beta_k H + \alpha_k V \\
  \Aim\Aiim &= HV + \beta_k\alpha_k I_n - \alpha_k H - \beta_k V
  .
\end{aligned}
\end{equation}

\begin{corollary}[Fully Commuting Splitting Scheme]
  The \ac{ADI} is a fully commuting parametrized splitting scheme if $H,V$ commute.
\end{corollary}
\begin{proof}
  %Obvious.
  $H$, $V$, $\gamma I_n$ commute for any $\gamma\in\C$, and thus $H^\pm_*, V^\pm_*$ commute.
\end{proof}

\begin{lemma}[Cayley Transformation]
\label{thm:adi:cayley}
  Let $A, E\in\Cnn$ and $\alpha, \beta \in\C$ with $-\alpha\notin\Lambda(A, E)$.
  The \emph{generalized Cayley transformation} of a matrix pair $(A,E)$ is defined by
  the rational matrix function
  \begin{equation*}
    \Cayley(A, E, \alpha, \beta)
    := (A+\alpha E)^{-1} (A-\beta E)
    = I_n - (\alpha + \beta) (A+\alpha E)^{-1} E
    .
  \end{equation*}
  Its eigenvalues are given by
  $(\lambda + \alpha)^{-1} (\lambda - \beta)$
  for $\lambda\in\Lambda(A, E)$.
  If $\Lambda(A, E) \subseteq \Cneg$ and $\alpha\in\Cneg$,
  then the spectral radius $\rho(\Cayley(A,E,\alpha,\conj\alpha)) < 1$.
\end{lemma}
\begin{proof}
  Compare \cite[Proposition~2.16]{Kuerschner2016}.
\end{proof}

Assuming that $H, V$ commute,
the iteration matrix $G_k$ for the \ac{ADI} is therefore given as a sequence of Cayley transformations,
\begin{equation}
\label{eq:adi:cayley}
  G_k
  := M_k^{-1} N_k
  = \underbrace{
    \big( \Aipinv\Aim \big)
  }_{
    \Cayley(H, I, \alpha_k, \beta_k)
  } \underbrace{
    \big( \Aiipinv\Aiim \big)
  }_{
    \Cayley(V, I, \beta_k, \alpha_k)
  }
  .
\end{equation}
As will be motivated in the next section, $\beta_k = \conj{\alpha_k}$.
In that case, if $\Lambda(H), \Lambda(V) \subseteq \Cneg$
and $\alpha_k \in \Cneg$ for all $k\in\N$, then $\rho(G_k) < 1$ for all $k$.
This is a reasonable starting point towards $G_k\cdots G_0 \to 0$,
\cf~\autoref{thm:adi:convergence},
but not a necessary condition.
However, if $H, V$ commute,
there exist parameters $\Set{(\alpha_k, \beta_k) \given k\in\N}$
such that the \ac{ADI} shows
superlinear convergence, see~\cite{Beckermann2010}.

\section{Application to \texorpdfstring{\act{ALE}s}{ALEs}}
\label{sec:adi:ale}

Consider the \ac{ALE}
\begin{equation}
\label{eq:adi:ale}
  \Lyap(X) := AX + XA^\HT = -W
\end{equation}
where $W = W^\T$.\footnote{%
\label{footnote:adi:transpose}
  Note that the position of the transpose differs from the usage in~\autoref{sec:ros},
  \eg~Equations~\eqref{eq:ros:DRE:ros1} and~\eqref{eq:ros:DRE:ros2}.
  Both variants are common and, hopefully, will not cause confusion.
  The notation in~\autoref{sec:ros} is closer to performant implementations using matrices in column-major storage,
  while the notation in this chapter makes the low-rank formulation a little less verbose.
}
The \Lyapunov operator $\Lyap \equiv \Lyap_L + \Lyap_R$ naturally decomposes into
left-multiplication $\Lyap_L : X \mapsto AX$ and
right-multiplication $\Lyap_R : X \mapsto XA^\HT$.
Furthermore, $\Lyap_L$ and $\Lyap_R$ commute:
\begin{equation}
  \Lyap_L \circ \Lyap_R \equiv \Lyap_R \circ \Lyap_L \equiv X \mapsto AXA^\HT
\end{equation}
All the operators above are linear and act on $\Rnn$.
Apply the \ac{ADI} in its intermediate-step formulation~\eqref{eq:adi:general2step}:
\begin{equation}
  \left\{
  \begin{aligned}
    (A + \alpha_k I_n) X_{k+\frac{1}{2}} &= -W - X_k (A^\HT - \alpha_k I_n) \\
    X_{k+1} (A^\HT + \beta_k I_n) &= -W - (A - \beta_k I_n) X_{k+\frac{1}{2}}
  \end{aligned}
  \right.
\end{equation}
The one-step formulation in its first normal form~\eqref{eq:adi:general1step} reads:
\begin{equation}
  (A + \alpha_k I_n)
  X_{k+1}
  (A + \conj{\beta_k} I_n)^\HT
  =
  (A - \beta_k I_n)
  X_k
  (A - \conj{\alpha_k} I_n)^\HT
  - (\alpha_k + \beta_k)
  W
\end{equation}
Choosing $\beta_k := \conj{\alpha_k}$
the above becomes Hermitian:
\begin{equation}
\label{eq:adi:ale1step}
  (A + \alpha_k I_n)
  X_{k+1}
  (A + \alpha_k I_n)^\HT
  =
  (A - \conj{\alpha_k} I_n)
  X_k
  (A - \conj{\alpha_k} I_n)^\HT
  - 2\Re(\alpha_k)
  W
\end{equation}

\begin{remark}
  Let $\alpha\in\C\setminus\R$ and $A_+ := A+\alpha I_n \in\Cnn$.
  As a mapping of matrices,
  $\Lyap_R + \alpha I_{n^2} : \Cnn \to \Cnn$ maps $U$ onto
  $
    U A^\HT + \alpha U =
    U(A+\conj{\alpha} I_n)^\HT \neq
    U A_+^\HT
  $.
  Therefore, the notation is slightly more involved than in the previous section.
  Refer to \autoref{sec:vectorization} for more details on how to represent the application of a linear operator as the left-multiplication by a matrix.
\end{remark}

\begin{remark}
  \citeauthor{Lang2017}~\cite{Lang2017} formulates the \ac{ALE}~\eqref{eq:adi:ale} in terms of the
  right-multiplication $\tilde \Lyap_R :X \mapsto XA^\T$, $\tilde \Lyap_R \neq \Lyap_R$.
  The \ac{ADI}~\cite[Equation~(2.23)]{Lang2017} uses a somewhat inconsistent second step.
  This leads to the impression that the \ac{ADI} is a single-parameter \ac{ADI} having $\tilde\beta_k := \tilde\alpha_k$
  instead of being a true two-parameter \ac{ADI} where potentially $\alpha_k \neq \beta_k := \conj{\alpha_k}$.
  The one-step formulation \cite[Equation~(2.24)]{Lang2017} is equivalent to Equation~\eqref{eq:adi:ale1step}.
\end{remark}

\subsection{Low-Rank Formulation}
\label{sec:adi:lr1step}

Define the short-hand notation
\begin{equation}
\label{eq:adi:ale:shorthand}
\begin{aligned}
  A^+_k &:= A + \alpha_k I_n \\
  A^-_k &:= A - \conj{\alpha_k} I_n
\end{aligned}
\end{equation}
and let the low-rank factorizations
\begin{equation}
\begin{aligned}
  X_k &= L_k D_k L_k^\HT \\
  W &= GSG^\HT
\end{aligned}
\end{equation}
for $G\in\R^{n\times n_G}$, $S\in\R^{n_G\times n_G}$ be given.
Typically, $n_G \ll n$.
Then, Equation~\eqref{eq:adi:ale1step} reads
\begin{equation}
  (A_k^+ L_{k+1}) D_{k+1} (A_k^+ L_{k+1})^\HT
  = (A_k^- L_k) D_k (A_k^- L_k)^\HT - 2\Re(\alpha_k) GSG^\HT
\end{equation}
such that
the \ac{ADI} can be stated in \ac{LRSIF} directly.
Using the update formulas of \autoref{sec:lowrank},
the iterates are build by successively adding column blocks of size $n_G$:
\begin{equation}
\begin{aligned}
  A_k^+ L_{k+1} &= \begin{bmatrix}
    A^-_k L_k &
    G
  \end{bmatrix} \\
  D_{k+1} &= \begin{bmatrix}
    D_k \\
    & -2\Re(\alpha_k) S
  \end{bmatrix}
\end{aligned}
\end{equation}
Using an initial value $X_0=0$,
the $k$-th iterate has dimensions $L_k \in\R^{n\times kn_G}$ and $D_k \in\R^{kn_G\times kn_G}$.
As a consequence, computing $L_{k+1}$
requires $kn_G + n_G$ system solves and
becomes increasingly expensive.
This can be remedied by a reformulation according to \autoref{thm:adi:increment-form}.

\pagebreak

In the notation of the previous section,
\begin{equation}
\label{eq:adi:ale:MN}
\begin{aligned}
  M_k &: U \mapsto \big( 2\Re(\alpha_k) \big)^{-1} (A_k^+) U (A_k^+)^\HT \\
  N_k &: U \mapsto \big( 2\Re(\alpha_k) \big)^{-1} (A_k^-) U (A_k^-)^\HT
\end{aligned}
\end{equation}
such that \autoref{thm:adi:increment-form} yields the following equivalent formulation of Equation~\eqref{eq:adi:ale1step}:
\begin{equation}
\left\{
\begin{aligned}
  X_{k+1}
    &= X_k + \tilde V_k, &
  \quad %FIXME
  A_k^+ \tilde V_k (A_k^+)^\HT
    &= \frac{2\Re(\alpha_k)}{2\Re(\alpha_{k-1})}
      (A_{k-1}^-) \tilde V_{k-1} (A_{k-1}^-)^\HT
  ,\\
  X_0
    &= 0, &
  A_0^+ \tilde V_0 (A_0^+)^\HT
    &= -2\Re(\alpha_0) W
  .
\end{aligned}
\right.
\end{equation}
Assume a factorization of the increment $\tilde V_k = V_k Y_k V_k^\HT$ for $k\in\N$.
Then, for $W = GSG^\HT$,
the low-rank version of the initial value $\tilde V_0$ reads
\begin{equation}
\begin{aligned}
  A_0^+ V_0 &= G, \\
  Y_0 &= -2\Re(\alpha_0) S.
\end{aligned}
\end{equation}
Thus,
the iteration of the increment $\tilde V_{k} = V_k Y_k V_k^\HT$ reads
\begin{equation}
\begin{aligned}
  A_k^+ V_k &= A_{k-1}^- V_{k-1} \\
  Y_k &= \frac{2\Re(\alpha_k)}{2\Re(\alpha_{k-1})} Y_{k-1}
  = -2\Re(\alpha_k) S
\end{aligned}
\end{equation}
such that the overall iteration $X_{k+1} = X_k + \tilde V_k$ is given by
\begin{equation}
\begin{aligned}
  L_{k+1} &= \begin{bmatrix}
    L_k &
    V_k
  \end{bmatrix} \\
    D_{k+1} &= \begin{bmatrix}
      D_k \\
      & Y_k
    \end{bmatrix}
    = \begin{bmatrix}
      D_k \\
      & -2\Re(\alpha_k) S
    \end{bmatrix}
\end{aligned}
\end{equation}
Summing up,
this leads to the low-rank procedure
\begin{equation}
\label{eq:adi:si-lr-adi}
\left\{
\begin{aligned}
  L_k &= \begin{bmatrix}
    V_0 &
    \cdots &
    V_{k-1}
  \end{bmatrix} \\
  D_k &= \begin{bmatrix}
    -2 \Re(\alpha_0) S \\
    & \ddots \\
    && -2 \Re(\alpha_{k-1}) S
  \end{bmatrix} \\
  A_k^+ V_k &= A_{k-1}^- V_{k-1} \\
  A_0^+ V_0 &= G
  ,
\end{aligned}
\right.
\end{equation}
where $V_k \in\R^{n\times n_G}$ has constant dimensions.
Therefore, only $n_G$ system solves are necessary at any iteration $k$ of the \ac{ADI}.
This method first appeared in the work of \citeauthor{Benner2009}~\cite[Section~5]{Benner2009}.

\pagebreak
\noindent
This raises a couple of questions,
that the following subsections aim to answer:
\begin{enumerate}
  \item
    How to deal with the growing storage demand of the low-rank factors $L_k$ and $D_k$?

    One may perform a column compression after any iteration using the technique described in \autoref{sec:lowrank}.
  \item
    When should the iteration stop?

    A traditional criterion is to stop when the residual reaches a certain threshold.
    This requires an efficient description of the residual.
  \item
    What happens if $\alpha_k\in\C\setminus\R$?

    Complex arithmetic is more expensive than real arithmetic.
    As will be discovered later, complex parameters occur in conjugated pairs.
    This allows a reformulation of the combined step,
    that only requires real arithmetic.
\end{enumerate}

\subsection{Low-Rank Stopping Criterion}
\label{sec:adi:lrstop}

\begin{theorem}[Low-Rank Lyapunov Residual]
\label{thm:adi:lowrank-residual}
  The residual of the \ac{ADI},
  with $\beta_k := \conj{\alpha_k}$ and $X_0=0$ applied to the \ac{ALE}~\eqref{eq:adi:ale},
  has a well-defined low-rank factorization compatible to $W = GSG^\HT$,
  $G\in\C^{n\times n_G}$, $S\in\C^{n_G\times n_G}$, namely
  \begin{equation*}
    \Lyap(X_k) + G S G^\HT = R_k S R_k^\HT
  \end{equation*}
  for $R_k \in\C^{n\times n_G}$ and $R_0 = G$.
  Furthermore, it holds
  \begin{align*}
    R_{k+1} &= R_k - 2\Re(\alpha_k) V_k \\
    V_k &= (A_k^+)^{-1} R_k
  \end{align*}
  where $V_k \in\C^{n\times n_G}$ denotes the low-rank increment of Formula~\eqref{eq:adi:si-lr-adi}.
\end{theorem}
\begin{proof}
  In the notation of the previous section,
  let $r^k$ and $v^k$ denote residual and increment, respectively.
  Obviously, $r^0 := \Lyap(0) + W = W = GSG^\HT$, hence $R_0 = G$.
  Due to \autoref{thm:adi:residual},
  \begin{equation*}
    r^{k+1}
    = G_k r^k
    = \big( (A_k^+)^{-1} A_k^- \big)
    R_k S R_k^\HT
    \big( (A_k^+)^{-1} A_k^- \big)^\HT
  \end{equation*}
  using the short-hand notation~\eqref{eq:adi:ale:shorthand},
  and the iteration mapping $G_k \equiv M_k^{-1} \circ N_k$ as defined by Equation~\eqref{eq:adi:ale:MN}.
  This implies
  \begin{equation*}
  \tag{$\ast$}
    R_{k+1}
    = \big( (A_k^+)^{-1} A_k^- \big) R_k
    = R_k - 2\Re(\alpha_k) (A_k^+)^{-1} R_k
    ,
  \end{equation*}
  using \autoref{thm:adi:cayley},
  \ie $R_k$ is well-defined $\forall k$.

  By definition of the increment,
  $v^k := -M_k^{-1} \big( r^k \big)$,
  \cf \autoref{thm:adi:increment-form},
  it holds
  \begin{equation*}
    V_k Y_k V_k^\HT
    = (A_k^+)^{-1} R_k
    \underbrace{
      \big( -2 \Re(\alpha_k) S \big)
    }_{Y_k}
    R_k^\HT (A_k^+)^{-\HT}
    ,
  \end{equation*}
  \ie $V_k = (A_k^+)^{-1} R_k$.
  Applying this to $(\ast)$ yields the remaining equality.
\end{proof}

\begin{remark}
  The curious reader may compare the proof above to \cite[Theorem~3.5]{Kuerschner2016}.
\end{remark}

Still, the full residual $R_k S R_k^\HT \in\Cnn$ should never be assembled.
Note that $R_k S R_k^\HT$ and $R_k^\HT R_k S \in \C^{n_G\times n_G}$ have the same spectra,
\cf~\cite[Remark on \pno~56]{Lang2015}.
Therefore, both are diagonalizable such that
\begin{equation}
\label{eq:adi:residual}
  \norm[\big]{R_k S R_k^\HT}_2
  = \rho\big( R_k S R_k^\HT \big)
  = \rho\big( R_k^\HT R_k S \big)
  = \norm[\big]{R_k^\HT R_k S}_2
  .
\end{equation}

Suppose the residual is real, \ie $R_k \in\R^{n\times n_G}$.
Using the well-known inequality
\begin{equation}
  \norm{X}_2
  \leq \norm{X}_F
  \leq \sqrt{r} \norm{X}_2
\end{equation}
for any matrix $X\in\Rnn$ where $r:=\rank X$,
one may replace the spectral norm in Equation~\eqref{eq:adi:residual} and \cite[Algorithm~2.2]{Lang2017}
by the Frobenius norm.
This leads to the stopping criterion
\begin{equation}
\label{eq:adi:lrstop}
  \norm[\big]{R_k^\HT R_k S}_F
  \leq
  \epsilon \norm[\big]{G^\HT G S}_F
\end{equation}
for a user-specified threshold $0<\epsilon \ll 1$.
Since $r \leq n_G$, this relation implies
\begin{equation}
  \norm[\big]{R_k^\HT R_k S}_2
  \leq
  \epsilon \sqrt{n_G} \norm[\big]{G^\HT G S}_2
  ,
\end{equation}
which is slightly weaker than the behavior of \citeauthor{Lang2017}'s implementation \cite{Lang2017}.
Replacing $\epsilon$ by $n_G^{-1/2} \epsilon$ leads to a slightly more restrictive stopping criterion compared to \cite{Lang2017}.

\subsection{Low-Rank Double-Step}
\label{sec:adi:lr2step}

\begin{proposition}[Complex Conjugated Parameters]
\label{thm:adi:ale:complex-pair}
  Suppose $A, W \in\Rnn$ and $X_k\in\Rnn$ is real for a fixed \mbox{$k\in\N$},
  \ie $G, R_k \in \R^{n\times n_G}$ and $S\in\R^{n_G\times n_G}$ as defined in \autoref{thm:adi:lowrank-residual}.
  If the next parameters form a complex conjugated pair,
  $\alpha_{k+1} = \conj{\alpha_k}$,
  then the iterate after that, $X_{k+2}$ is real again.
  More specifically,
  \begin{align*}
    R_{k+2} &= R_k - 4\Re(\alpha_k) \big(
      \Re(V_k) + \delta_k \Im(V_k)
    \big) \\
    V_{k+1} &= \conj{V_k} + 2\delta_k \Im(V_k)
  \end{align*}
  for $\delta_k := \Re(\alpha_k) / \Im(\alpha_k)$.
\end{proposition}
\begin{proof}
  Compare \citeauthor{Kuerschner2016}~\cite[Theorem~4.2]{Kuerschner2016}.
\end{proof}

\begin{corollary}[Low-Rank Double-Step]
  Suppose that $L_k, D_k$ are real-valued for a fixed $k\in\N$.
  If the next parameters form a complex conjugated pair,
  $\alpha_{k+1} = \conj{\alpha_k}$,
  an equivalent low-rank update is given by
  \begin{equation*}
    L_{k+2} = \begin{bmatrix}
      L_k &
      \hat V_k &
      \hat V_{k+1}
    \end{bmatrix}
  \end{equation*}
  where
  \begin{align*}
    \hat V_k &= \sqrt{2} \big( \Re(V_k) + \delta_k \Im(V_k) \big) \\
    \hat V_{k+1} &= \sqrt{\smash[b]{2\big( \delta_k^2 + 1 \big)}} \Im(V_k)
  \end{align*}
  and $D_{k+2}$ remains as in Equation~\eqref{eq:adi:si-lr-adi}.
\end{corollary}
\begin{proof}
  The double-step reads
  \begin{equation*}
    X_{k+2} = X_k
    + V_k Y_k V_k^\HT
    + V_{k+1} Y_{k+1} V_{k+1}^\HT
  \end{equation*}
  where $Y_k = -2 \Re(\alpha_k) S = Y_{k+1}$,
  since $\Re(\alpha_k) = \Re(\alpha_{k+1})$.
  By \autoref{thm:adi:ale:complex-pair},
  the combined increment is given by
  \begin{align*}
    \MoveEqLeft
    V_k Y_k V_k^\HT + V_{k+1} Y_{k+1} V_{k+1}^\HT
    \\
    &= V_k Y_k V_k^\HT +
    \big( \conj{V_k} + 2\delta_k \Im(V_k) \big)
    Y_k
    \big( \conj{V_k} + 2\delta_k \Im(V_k) \big)^\HT
    \\
    &= \begin{aligned}[t]
      & V_k Y_k V_k^\HT + \big( \conj{V_k} \big) Y_k \big( \conj{V_k} \big)^\HT \\
      & + 2\delta_k \big( \conj{V_k} \big) Y_k \Im(V_k)^\HT \\
      & + 2\delta_k \Im(V_k) Y_k \big( \conj{V_k} \big)^\HT \\
      & + 4\delta_k^2 \Im(V_k) Y_k \Im(V_k)^\HT
    \end{aligned}
    \\
    &= \begin{alignedat}[t]{4}
      &&                 2 \Re(V_k) & Y_k \Re(V_k)^\HT &
      &{}+{}&            2 \Im(V_k) & Y_k \Im(V_k)^\HT
      \\
      &{}+{}&    2\delta_k \Re(V_k) & Y_k \Im(V_k)^\HT &
      &{}-{}& 2\im\delta_k \Im(V_k) & Y_k \Im(V_k)^\HT
      \\
      &{}+{}&    2\delta_k \Im(V_k) & Y_k \Re(V_k)^\HT &
      &{}+{}& 2\im\delta_k \Im(V_k) & Y_k \Im(V_k)^\HT
      \\
      &{}+{}&  2\delta_k^2 \Im(V_k) & Y_k \Im(V_k)^\HT &
      &{}+{}&  2\delta_k^2 \Im(V_k) & Y_k \Im(V_k)^\HT
      ,
    \end{alignedat}
    \\
\intertext{which by grouping the terms in each column yields}
    &= \begin{multlined}[t]
      2 \big( \Re(V_k) + \delta_k \Im(V_k) \big)
      Y_k \big( \Re(V_k) + \delta_k \Im(V_k) \big)^\HT
      \\
      + 2\big( \delta_k^2 + 1 \big) \Im(V_k) Y_k \Im(V_k)^\HT
    \end{multlined}
    \\
    &= \hat V_k Y_k \hat V_k^\HT
    + \hat V_{k+1} Y_{k+1} \hat V_{k+1}^\HT
    .
  \end{align*}
  This last formula requires only real arithmetic,
  in nice analogy to the real-valued residual of \autoref{thm:adi:ale:complex-pair}.
\end{proof}

\section{Parameter Selection}
\label{sec:adi:parameters}

After deriving a suitable structure of the \ac{ADI} for the \ac{ALE},
the shift parameters $\Set{\alpha_k : k\in\N}$ are still to be determined.
Classical choices of \emph{pre-computed} values include
optimal Wachspress shifts \cite{Wachspress1992,Wachspress2013} and
heuristic Penzl shifts \cite{Penzl1999}.
These try to globally minimize $\rho(G_K \cdots G_0)$ for a fixed number of \ac{ADI} steps $K\in\N$,
and estimates for the spectra~$\Lambda(H)$ and~$\Lambda(V)$.
This thesis employs the first ansatz of \emph{self-generating shifts} described by \citeauthor{Kuerschner2016}~\cite[Section~5.3]{Kuerschner2016},
so-called $V(u)$-shifts,
which aim at minimizing
$\rho(G_k |_{\colspan V_k})$
separately for each iteration $k$,
which does not require any information on the spectra of $H$ or $V$.

As mentioned at the end of \autoref{sec:adi:1step},
a local criterion like this does in general not guarantee convergence.
However, these parameters seemed to outperform the aforementioned pre-computed approaches in the experiments of \cite{Kuerschner2016}.

The general idea is to restrict $A$ onto
$\mathcal Q := \colspan N \subseteq \C^n$
for some matrix $N$ and take its (stable) eigenvalues.
These so-called \emph{Ritz values} are defined as
\begin{equation}
  \Lambda\big( Q^\HT A Q, Q^\HT E Q \big) \cap \Cneg
  ,
\end{equation}
where $Q\in\C^{n\times d}$ is an orthonormal basis of $\mathcal Q$, $d := \dim\mathcal Q$.
In the context of $V(u)$-shifts for \ac{LRSIF} \ac{ADI} \cite{Lang2015}, choose
\begin{equation}
  N := \begin{bmatrix}
    V_{j-1} &
    \cdots &
    V_{j-u}
  \end{bmatrix}
  \in \R^{n\times u n_G}
  .
\end{equation}
The overall procedure is summarized in \autoref{alg:ADI}.\footnote{%
  \autoref{alg:ADI} positions the transposes as the previous chapter does,
  \cf~\autoref{footnote:adi:transpose} on \autopageref{footnote:adi:transpose}.
}

\begin{remark}
  Due to the low-rank structure, only that part of the iteration mapping \eqref{eq:adi:cayley} is considered,
  which operates on one of the low-rank factors.
  This part is exactly \mbox{$\hat G_k := \Cayley(A, E, \alpha_k, \conj{\alpha_k})$}.
  Suppose $\lambda \in \Lambda(A, E) \cap \R$, $\lambda < 0$.
  Notice how choosing $\alpha = \beta := \lambda$ in \autoref{thm:adi:cayley}
  maps $\lambda$ onto $0$ as an eigenvalue of $\Cayley(A, E, \lambda, \lambda)$.
  Above Ritz values act similarly on $\hat G_k |_{\colspan N}$.
  % TODO: Future research question: how does the ADI using these parameters relate to Krylov methods,
  % given the relation between increment and residual?
\end{remark}

To obtain an orthonormal basis,
recall that $\R^n$ and $\R^{un_G}$ decompose into
\begin{equation}
\label{eq:adi:param:ker+colspan}
\begin{aligned}
  \R^{un_G} = \ker N \oplus \colspan N^\T \\
  \R^n = \ker N^\T \oplus \colspan N
\end{aligned}
\end{equation}
and that $NN^\MP = N (N^\T N)^\MP N^\T$ is the projection onto $\colspan N$,
where $(\optional{})^\MP$ denotes the Moore-Penrose pseudo-inverse \cite{Strang2016}.
This leads to an implicit orthogonalization procedure.
Note that $N^\T N$ is symmetric positive semi-definite.
Hence, it permits a real eigendecomposition
\begin{align*}
  N^\T N &= \hat N \hat D \hat N^\T, &
  \hat N^\T \hat N &= I_r, &
  \hat D &= \diag(\lambda_1, \ldots, \lambda_r), &
  \lambda_1 \geq \ldots \geq \lambda_r > 0,
\end{align*}
where $r = \rank N = \rank N^\T N \leq u n_G$.
The Moore-Penrose pseudo-inverse is then given by \mbox{$(N^\T N)^\MP := \hat N \hat D^{-1} \hat N^\T$}.
Finally,
\begin{equation}
\label{eq:adi:param:Q}
  Q := N \hat N \hat D^{-\frac{1}{2}}
  \in\R^{n\times r}
\end{equation}
is an orthonormal basis of $\mathcal Q$.
First, $Q$ is orthonormal:
\begin{equation*}
  Q^\T Q
  = \hat D^{-\frac{1}{2}} \hat N^\T (N^\T N) \hat N \hat D^{-\frac{1}{2}}
  = \hat D^{-\frac{1}{2}} (\hat N^\T \hat N) \hat D (\hat N^\T \hat N) \hat D^{-\frac{1}{2}}
  = \hat D^{-\frac{1}{2}} \hat D \hat D^{-\frac{1}{2}}
  = I_r
\end{equation*}
Second, $\colspan N = \colspan Q$.
By \eqref{eq:adi:param:ker+colspan},
for any $x\in\R^{un_G}$ it is
\begin{align*}
  Nx
  &= NN^\T y + 0 &&\text{for some $y\in\R^n$} \\
  &= N(N^\T N \hat x + 0) && \text{for some $\hat x \in\R^{un_G}$} \\
  &= N (\hat N \hat D \hat N^\T) \hat x \\
  &= N \hat N \hat D^{-\frac{1}{2}} \hat y && \text{for $\hat y := \hat D^{\frac{3}{2}} \hat N^\T \hat x \in \R^r$}
  ,
\end{align*}
\ie $\colspan N \subseteq \colspan Q$.
This implies equality,
since $\colspan Q \subseteq \colspan N$ (trivial).

\pagebreak

The derivation above neglected zero eigenvalues of $N^\T N$,
whose eigenvectors are syzygies of $\colspan N$.
Indeed, let $(\epsilon,s)\in\R\times\R^{un_G}$ be an eigenpair of $N^\T N$ with $\norm{s}_2 = 1$.
Then,
\begin{equation}
  \norm{Ns}_2^2
  = s^\T (N^\T N s)
  = \epsilon\norm{s}_2^2
  = \epsilon
  ,
\end{equation}
which for $\epsilon=0$ reveals linearly dependent columns of $N$.
In that sense, small eigenvalues $\lambda_*$ correspond to nearly linearly dependent columns.
It is therefore advised to only consider the first $d \leq r$ eigenvalues
\begin{equation}
  \lambda_* \geq \epsilon \lambda_1
\end{equation}
for some $0 < \epsilon \ll 1$,
\eg a multiple of the machine precision,
$\epsilon := n\umach$.
This corresponds to replacing
$\hat N$ by its first $d$ columns,
$\hat D$ by its leading diagonal block of size $d$,
and $\colspan N$ by $\colspan(\hat N \diag(\lambda_1, \ldots, \lambda_d, 0, \ldots) \hat N^\T)$,
while the overall formula \eqref{eq:adi:param:Q} remains unchanged.
Thus, the procedure leads to $d \leq r \leq un_G$ new shifts.

\begin{algorithm}[tp]
  %TODO: rename this as to not be confused with LR-ADI = LRCF-ADI
  \caption{Low-Rank Alternating-Directions Implicit Method}
  \label{alg:ADI}
  \KwIn{%
    Matrices $E, A, G, S$ defining a generalized \ac{ALE}~\eqref{eq:basics:ALE} where $W = GSG^\T$,
    tolerance $0 < \epsilon \ll 1$
    %iteration cap $J\in\N$ %TODO: reformulate chapter for index j
  }
  \KwOut{%
    $L, D$ such that $X \approx LDL^\T$
  }
  \Comment{Compute initial parameters, \cf \autoref{sec:adi:parameters}:}
  $Q \gets \orth G$\;
  $ \Set{\alpha_0, \alpha_1, \ldots} \gets \Lambda\big( Q^\HT A Q, Q^\HT E Q \big) \cap \Cneg$\;
  \Comment{Perform actual \ac{ADI}:}
  $R_0 \gets G$\;
  $k \gets 0$\;
  % Watch out: k as in the final condition already has the value for the next/upcoming iteration,
  % therefore subtract one:
  \Repeat{$\norm[\big]{R_{k}^\T R_{k} S}_F \leq \epsilon \norm[\big]{G^\T GS}_F$}{%
  %TODO: ...or $j > J$
    \If{$\alpha_k$ is not available}{%
      $Q \gets$ \leIf{$\alpha_{k-1}\in\R$}{%
        $\orth V_{k-1}$%
      }{%
        $\orth \begin{bmatrix}
          \hat V_{k-2} &
          \hat V_{k-1}
        \end{bmatrix}$
      }
      $\Set{\alpha_k, \alpha_{k+1}, \ldots} \gets \Lambda\big( Q^\HT A Q, Q^\HT E Q \big) \cap \Cneg$\;
    }
    $Y_k \gets -2\Re(\alpha_k) S$\;
    $A^+_k \gets A^\T + \alpha_k E$\; \nllabel{alg:ADI:smw}
    \eIf{$\alpha_k \in\R$}{%
      \Comment{Perform single step, \cf Subsections~\ref{sec:adi:lr1step} and \ref{sec:adi:lrstop}:}
      Solve $A^+_k V_k = R_k$ for $V_k$\;
      $L_{k+1} \gets \begin{bmatrix}
        L_k &
        V_k
      \end{bmatrix}$,\quad
      $D_{k+1} \gets \begin{bmatrix}
        D_k \\
        & Y_k
      \end{bmatrix}$\;
      $R_{k+1} \gets R_{k} - 2\alpha_k E V_k$\;
      $k \gets k+1$\;
    }{%
      \Comment{Perform double-step, \cf \autoref{sec:adi:lr2step}:}
      Solve $A^+_k V_k = R_k$ for $V_k$\;
      $\delta_k \gets \Re(\alpha_k) / \Im(\alpha_k)$\;
      $\hat V_k \gets \sqrt{2} \big( \Re(V_k) + \delta_k \Im(V_k) \big)$\;
      $\hat V_{k+1} \gets \sqrt{\smash[b]{2\big( \delta_k^2 + 1 \big)}} \Im(V_k)$\;
      $L_{k+2} \gets \begin{bmatrix}
        L_k &
        \hat V_k &
        \hat V_{k+1}
      \end{bmatrix}$,\quad
      $D_{k+2} \gets \begin{bmatrix}
        D_k \\
        & Y_k \\
        && Y_k
      \end{bmatrix}$\;
      $R_{k+2} \gets R_{k} - 4\Re(\alpha_k) E \big( \Re(V) + \delta_k \Im(V) \big)$\;
      $ k \gets k + 2$\;
    }
    Compress $L_{k}, D_{k}$ if necessary, \cf~\autoref{alg:lowrank:compression}\;
  }(,~\cf \autoref{sec:adi:lrstop})
\end{algorithm}

%\section{Alternative Lyapunov Solvers}

%A different approach to solve \ac{ALE} are Krylov subspace methods,
%see \eg~\cite{Simoncini2016}.

% projection-based?
% hybrid?

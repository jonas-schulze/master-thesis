\chapter{Alternating-Directions Implicit Method}

The \ac{ADI} method goes back to \cite{Peaceman1955}.
It has originally been designed to solve linear systems
$Ax=b$
for symmetric positive-definite $A\in\Rnn$
and applied to elliptic and parabolic \acp{PDE} using a finite-difference scheme.
In this chapter, after stating the classical parametrized intermediate-/two-step formulation,
we describe its properties as a one-step method.
Next, we will apply the method to \ac{ALE} in \autoref{sec:adi:ale},
and determine suitable parameters in \autoref{sec:adi:parameters}.
Finally, we will mention some alternative solvers for \ac{ALE}.

Suppose $A=A_1+A_2$ and $x\in\R^n$ is a solution to $Ax=b$.
Then it is easy to see that
\begin{align*}
  (A_1 + \alpha I_n)x &= b - (A_2 - \alpha I_n) x \\
  (A_2 + \beta I_n)x &= b - (A_1 - \beta I_n) x
\end{align*}
for any $\alpha, \beta \in\C$.
Define the short-hand notation
\begin{equation}
\label{eq:adi:shorthand}
\begin{aligned}
  \Aip  &:= A_1 + \alpha_k I_n &
  \qquad\qquad\qquad %FIXME
  \Aiip &:= A_2 + \beta_k  I_n \\
  \Aim  &:= A_1 - \beta_k  I_n &
  \Aiim &:= A_2 - \alpha_k I_n
\end{aligned}
\end{equation}
and convert the above into an iteration scheme
\begin{equation}
  \label{eq:adi:general2step}
  \left\{
  \begin{aligned}
    \Aip  x^{k+\frac{1}{2}} &= b - \Aiim x^k \\
    \Aiip x^{k+1}           &= b - \Aim x^{k+\frac{1}{2}}
  \end{aligned}
  \right.
\end{equation}
where the parameters $\alpha_k, \beta_k \in\C$ are free to choose for every iteration $k\in\N$.

\section{ADI as a Linear Splitting Scheme}
\label{sec:adi:1step}

Many iterative methods solving $Ax=b$ can be written in a one-step \emph{splitting} form
\begin{equation}
\label{eq:adi:1nf}
  Mx^{k+1} = Nx^k + b
\end{equation}
where $M-N = A$ and $M$ is \enquote{easy} to invert \cite[Section~11.2.3]{Golub2013}.
These methods are \emph{consistent} by construction,
\ie every solution $\hat x$ to $Ax=b$ is a fixpoint of the iteration.
Conversely, every fixpoint $x^{k+1} = x^k$ is a solution as well.
\todo{The proof may need to be adjusted for variable $G_k$.}
The method converges if $\rho(G) < 1$ for $G:=M^{-1}N$ \cite[Theorem~11.2.1]{Golub2013}.
$G$ is called \emph{iteration matrix} of the scheme.

Obviously, every sub-step of the \ac{ADI}~\eqref{eq:adi:general2step} is a splitting scheme.
First, we will show how this property is passed onto its one-step formulation.
Then, we will provide an alternative formulation of the ADI following \cite{Li2002} that simplifies computations.
Finally, we will analyse the convergence.

\begin{remark}
  \eqref{eq:adi:1nf} is often called \emph{first normal form} of a linear one-step method.
  \todo{Find reference. Until then, see Warnecke lecture notes, Lemma~3.5.}
  Consistency is then equivalent to $M-N=A$.
  The \emph{second normal form} of a consistent linear iteration method reads
  $
    x^{k+1}
    = M^{-1} \big( (M-A) x^k + b \big)
    = x^k - M^{-1}(Ax^k - b)
  $.
\end{remark}

\subsection{Computation}

Obviously, $\Aip\Aim = \Aim\Aip$.
Thus, by \autoref{thm:adi:commuting-matrices} below, $\Aim\Aipinv = \Aipinv\Aim$.
Substituting the intermediate step $x^{k+\frac{1}{2}}$ into the final part of \eqref{eq:adi:general2step} leads to
\begin{equation*}
  \Aiip x^{k+1}
  = b - \underbrace{
    \Aim\Aipinv
  }_{\Aipinv\Aim}
  (b - \Aiim \hat x^k)
  .
\end{equation*}
Multiplying by $\Aip$ then gives
\begin{equation*}
  \Aip\Aiip x^{k+1} = \Aim\Aiim x^k +
  \underbrace{%
  (\Aip - \Aim)
  }_{(\alpha_k + \beta_k) I_n}
  b
  .
\end{equation*}
Finally, multiplying by $(\alpha_k + \beta_k)^{-1}$ shows
\begin{equation}
\label{eq:adi:general1step}
  \underbrace{
    (\alpha_k + \beta_k)^{-1} \Aip\Aiip \\
  }_{M_k}
  x^{k+1} =
  \underbrace{
    (\alpha_k + \beta_k)^{-1} \Aim\Aiim
  }_{N_k}
  x^k + b
  .
\end{equation}
Indeed, $M_k - N_k = A$ since
\begin{align*}
  \Aip\Aiip &= A_1A_2 + \alpha_k\beta_k I_n + \beta_k A_1 + \alpha_k A_2 \\
  \Aim\Aiim &= A_1A_2 + \beta_k\alpha_k I_n - \alpha_k A_1 - \beta_k A_2
  .
\end{align*}

\begin{lemma}[Commuting Matrices]
\label{thm:adi:commuting-matrices}
  Let $\alpha, \beta\in\C$ and let $A, B \in\Cnn$ commute.
  If $A$ is invertible, then $A^{-1}B = BA^{-1}$.
  If $(A+\alpha I_n)$ is invertible, then
  \begin{equation*}
    (A+\alpha I_n)^{-1} (B-\beta I_n)
    = (B-\beta I_n) (A+\alpha I_n)^{-1}
    .
  \end{equation*}
  Furthermore, for $M\in\Cnn$ it holds
  \begin{equation*}
    (A+\alpha M)^{-1} (A-\beta M)
    = I_n - (\alpha+\beta) (A+\alpha M)^{-1} M
    .
  \end{equation*}
\end{lemma}
\begin{proof}
  Multiplying $AB=BA$ with $A^{-1}$ from either side yields $A^{-1}B=BA^{-1}$.
  Furthermore, $AB=BA$ implies
  $
    (A+\alpha I_n) (B-\beta I_n)
    =
    (B-\beta I_n) (A+\alpha I_n)
  $.
  Applying the first claim yields the desired equality.
  The last claim follows from noting
  $A-\beta M = (A+\alpha M) - (\alpha+\beta)M$
  and multiplying by $(A+\alpha M)^{-1}$.
\end{proof}

\begin{proposition}[Independence of Order of Parameters]
\label{thm:adi:permutation}
  Let $A=A_1 + A_2$ be given for commuting $A_1,A_2$.
  Further, fix $K\in\N$ and parameters $\Set{(\alpha_k, \beta_k) \given 1 \leq k \leq K} \subseteq \C^2$.
  Define the \ac{ADI} $x^{k+1} := M_k x^k + N_k b$ according to \eqref{eq:adi:general1step}.
  Then, the value of $x^K$ does not depend on the order of the parameters.
\end{proposition}
\begin{remark}
  Any previous $x^k$, $k < K$, may take a different value depending on which parameters
  $\Set{(\alpha_\ell,\beta_\ell) \given k < \ell \leq K}$ are yet to be chosen.
\end{remark}
\begin{proof}
  The following is a generalization of the proof of \cite[Theorem~4.1]{Li2002}.
  Let $G_k := M_k^{-1} N_k$ denote the iteration matrix
  and $B_k := M_k^{-1}$ denote the matrix of the second normal form.
  By \autoref{thm:adi:commuting-matrices}, all the matrices $A_1, A_2, G_*, B_*$ commute.
  Therefore, any neighboring parameters $(\alpha_k,\beta_k)$ and $(\alpha_{k-1},\beta_{k-1})$ may be exchanged,
  \begin{align*}
    x^{k+1}
    &= G_k x^k + B_k b \\
    &= G_k (G_{k-1} x^{k-1} + B_{k-1} b) + B_k b \\
    &= \underbrace{
      (G_k G_{k-1})
    }_{
      G_{k-1} G_k
    } x^{k-1} + \underbrace{
      (G_k B_{k-1} + B_k)
    }_{
      G_{k-1} B_k + B_{k-1}
    } b
  \end{align*}
  since $N_k = M_k - A$ implies $G_k = I_n - B_k A$ as well as
  \begin{align*}
    G_k B_{k-1} + B_k
    &= (I_n - B_k A) B_{k-1} + B_k \\
    &= B_{k-1} - B_k (A_1+A_2) B_{k-1} + B_k \\
    &= B_{k-1} + (I_n - B_{k-1} A) B_k \\
    &= B_{k-1} + G_{k-1} B_k
    .
  \end{align*}
  As any permutation of $(\alpha_k,\beta_k)_{1\leq k \leq K}$ may be obtained by exchanging neighboring parameters,
  $x^K$ does thus not depend on the order of the parameters.
\end{proof}

This allows a reformulation of the \ac{ADI} to simplify computations \cite{Li2002}.
The exact motivation will be given in the next section.
For an initial value of $x^0 = 0$,
the ADI reads:
\begin{align*}
  x^{k+1}
  &= G_k x^k + B_k b \\
  &= G_k G_{k-1} x^{k-1} + (G_k B_{k-1} + B_k) b \\
  &= G_k G_{k-1} (G_{k-2} x^{k-2} + B_{k-2}b) + (G_k B_{k-1} + B_k) b \\
  &= G_k \cdots G_{k-2} x^{k-2} + (G_k G_{k-1} B_{k-2} + G_k B_{k-1} + B_k) b \\
  &\vdotswithin{=} \\
  &= \underbrace{G_k \cdots G_0 x^0}_0 {} + (G_k \cdots G_1 B_0 + \ldots + G_k B_{k-1} + B_k) b \\
\intertext{%
  By \autoref{thm:adi:permutation} we may introduce a permutation $\sigma$ of $\Set{0,\ldots,k}$.
  Thus:
}
  &= (G_{\sigma(k)} \cdots G_{\sigma(1)} B_{\sigma(0)} + \ldots + G_{\sigma(k)} B_{\sigma(k-1)} + B_{\sigma(k)}) b \\
\intertext{%
  Choosing $\sigma = \sigma_k : i \mapsto k-i$,
  and exploiting that $G_* B_* = B_* G_*$ commute,
  yields:
}
  &= (G_0 \cdots G_{k-1} B_k + \ldots + G_0 B_1 + B_0) b \\
  &= (B_k G_{k-1} \cdots G_0 + \ldots + B_1 G_0 + B_0) b \\
\intertext{%
  Further utilizing $G_k := M_k^{-1} N_k = N_k M_k^{-1}$ and $B_k := M_k^{-1}$ leads to:
}
  &= \big( M_k^{-1} (N_{k-1} M_{k-1}^{-1}) \cdots (N_0 M_0^{-1}) + \ldots + M_1^{-1} (N_0 M_0^{-1}) + M_0^{-1} \big) b \\
  &= \big( (M_k^{-1} N_{k-1}) \cdots (M_1^{-1} N_0) M_0^{-1} + \ldots + (M_1^{-1} N_0) M_0^{-1} + M_0^{-1} \big) b
\end{align*}
\todo{Using $v^k$ instead of $v^{k+1}$ is unfortunate, but the other one hides too much information.}
Regrouping reveals:
\begin{equation}
\label{eq:adi:recursion}
\left\{
\begin{aligned}
  x^{k+1} &= x^k + v^k \\
  v^k &= (M_k^{-1} N_{k-1}) v^{k-1} \\
  x^0 &= 0 \\
  v^0 &= M_0^{-1} b
\end{aligned}
\right.
\end{equation}

\begin{remark}
  In accordance with the second normal form,
  $x^{k+1} = x^k - M_k^{-1} (Ax^k - b)$, it is
  $v^k = - M_k^{-1} (Ax^k - b)$ and
  $N_{k-1} v^{k-1} = Ax^k - b$.
\end{remark}

Nothing of the above is special to the \ac{ADI} method.
In fact, all that was necessary in the derivation above as well as the proof of \autoref{thm:adi:permutation} was that $M_*$ and $N_*$ commute.

\begin{corollary}[Recursive Structure of Residual]
  Let $M_k x^{k+1} = N_k x^k + b$ be some parametrized linear method to solve $Ax=b$.
  Let the method be consistent in the sense that $M_k - N_k = A$ for any $k\in\N$.
  Let the initial value be $x^0=0$.
  Then, the residual $r_k := Ax^k - b$ as in the method's second normal form,
  \begin{equation*}
    x^{k+1} = x^k - M_k^{-1} (Ax^k - b) = x^k - M_k^{-1} r^k
  \end{equation*}
  adheres to
  \begin{equation*}
  \left\{
  \begin{aligned}
    r^k &= N_{k-1} M_{k-1}^{-1} r^{k-1} \\
    r^0 &= -b
  \end{aligned}
  \right.
  \end{equation*}
\end{corollary}
\begin{proof}
  Obviously, $r^0 = 0-b = -b$.
  As we have already seen, $v^k = -M_k^{-1} r^k$ for any $k\in\N$,
  and $v^k = M_k^{-1} N_{k-1} v^{k-1}$ as in \eqref{eq:adi:recursion}.
  Hence:
  \begin{equation*}
    r^k
    = -M_k v^k
    = -N_{k-1} v^{k-1}
    = N_{k-1} M_{k-1}^{-1} r^{k-1}
  \end{equation*}
\end{proof}

\todo[inline]{%
  The identity in the Corollary above is equivalent to \eqref{eq:adi:recursion}.
  The proof can be redone without it, leading to an overall simpler proof of \eqref{eq:adi:recursion}:
}

\begin{proof}
  Due to $N_k = M_k - A$ it is $I - AM_k^{-1} = N_k M_k^{-1}$.
  Furthermore, as $M_k$ and $N_k$ commute,
  \begin{align*}
    r^{k+1}
    &= Ax^{k+1} - b \\
    &= A M_k^{-1} (N_k x^k + b) - b \\
    &= M_k^{-1} N_k (\underbrace{
      \vphantom{M_k^{-1}}
      Ax^k - b
    }_{
      \vphantom{M_k^{-1}}
      r^k
    } + b) + \underbrace{
      A M_k^{-1} b - b
    }_{
      -(I - AM_k^{-1})b
    }\\
    &= M_k^{-1} N_k r^k + \underbrace{
      M_k^{-1} N_k b - N_k M_k^{-1} b
    }_0
    .
  \end{align*}
\end{proof}

Above proof doesn't even require $x^0=0$,
as the derivation of \cite{Li2002} did.%
\footnote{%
  This will remain handy for \ac{ALE},
  since any factorization of $b$ is passed onto $x^1$ and thereby $x^k$.
}
Rather, it is the mere observation that the iteration matrices $G_k := M_k^{-1} N_k$
not only determine the behavior of the error \cite[Equation~(11.2.7)]{Golub2013},
but the behavior of the residual as well.\footnote{Does this even deserve a theorem of its own?}
Hence, by the second normal form of a consistent linear method,
\begin{equation*}
  x^{k+1} = x^k - M_k^{-1} r^k =: x^k + v^k
  \qquad
  \forall k\in\N
\end{equation*}
we observe the recursion
\begin{equation*}
  v^k
  = -M_k^{-1} r^k
  = -M_k^{-1} N_{k-1} M_{k-1}^{-1} r^{k-1}
  = M_k^{-1} N_{k-1} v^{k-1}
\end{equation*}
with $v^0 = -M_0^{-1} r^0 = M_0^{-1} (b-Ax^0)$,
which is a generalization of \eqref{eq:adi:recursion}.

\subsection{Convergence}

Such an iteration converges iff $\rho(M_k) < 1$, $\forall k\in\N$ \cite[Theorem~11.2.1]{Golub2013}.
We restrict the analysis to commuting $A_1, A_2$.
In this case,
there exist parameters $\Set{(\alpha_k, \beta_k) \given k\in\N}$
such that the \ac{ADI} shows
\todo{Can I show this?}
superlinear convergence \cite{Beckermann2010}.

\begin{lemma}[Cayley Transformation]
\label{thm:adi:cayley}
  Let $A, M\in\Cnn$ and $\alpha, \beta \in\C$ with $-\alpha\notin\Lambda(A, M)$.
  The \emph{generalized Cayley transformation} of a matrix pair $(A,M)$ is defined by
  the rational matrix function
  \begin{equation*}
    \Cayley(A, M, \alpha, \beta) := (A+\alpha M)^{-1} (A-\beta M).
  \end{equation*}
  \todo[inline]{
    Furthermore, $\Lambda(A,M) \subset\C_- \implies \rho(\Cayley(A,M,\alpha,\conj\alpha)) < 1$,
    \cf \cite[Proposition~2.16]{Kuerschner2016}.
  }
\end{lemma}
\begin{remark}
  In the context of control theory,
  the last claim states that
  $\Cayley(A, M, \alpha, \conj\alpha)$ is d-stable if
  $(A, M)$ is c-stable.
\end{remark}
\begin{proof}
\end{proof}

\begin{lemma}
  Let $\alpha_k,\beta_k\in\C$ and $A=A_1+A_2$ be given.
  Define $\Aipm, \Aiipm$ according to \eqref{eq:adi:shorthand}.
  The \ac{ADI} iteration matrix $M_k^{-1}N_k$ as in \eqref{eq:adi:general1step} is a Cayley transformation.
\end{lemma}
\begin{proof}
  For brevity, we omit the subscript $k$.
  Observe
  \begin{align*}
    \Aip\Aiip
    &= A_1A_2 + \alpha\beta I_n + \beta A_1 + \alpha A_2 \\
    &= A_1A_2 + \alpha\beta I_n - (\alpha-\beta)A_1 + \alpha(A_1 + A_2) \\
    \Aim\Aiim
    &= A_1A_2 + \alpha\beta I_n - \alpha A_1 - \beta A_2 \\
    &= A_1A_2 + \alpha\beta I_n - (\alpha-\beta)A_1 - \beta(A_1 + A_2)
  \end{align*}
  such that
  \todo{Check whether $-\alpha\in\Lambda(B, A)$.}
  \begin{equation*}
    M^{-1}N
    = (\Aip\Aiip)^{-1}(\Aim\Aiim)
    = \Cayley(B, A_1+A_2, \alpha, \beta)
  \end{equation*}
  for
  \begin{equation*}
    B := A_1A_2 + \alpha\beta I_n - (\alpha-\beta)A_1
    .
  \end{equation*}
\end{proof}

If $A_1, A_2$ commute,
all the matrices $\Aip,\Aim,\Aiip,\Aiim$ commute.
By \autoref{thm:adi:commuting-matrices} this extends to their inverses.
Thus,
\begin{equation*}
  M =
  \Aipinv\Aim
  \Aiipinv\Aiim
  =:
  \Cayley(A_1, I_n, \alpha, \beta)
  \cdot
  \Cayley(A_2, I_n, \beta, \alpha)
  ,
\end{equation*}
\todo{Does this help with $\rho(M)<1$?}
\ie $M$ is even a sequence of Cayley transformations.

\begin{hypothesis}[Convergence of ADI]
\label{thm:adi:convergence}
\todo{Rephrase in terms of eigenvalues}
  If $A_1$ and $A_2$ are stable, then $\rho(M) < 1$.
\end{hypothesis}

\section{Application to \act{ALE}s}
\label{sec:adi:ale}

Consider the \ac{ALE}
\begin{equation*}
\label{eq:adi:ale}
  L(X) := AX + XA^\HT = -W
\end{equation*}
where $W = W^\T$.
The \Lyapunov operator $L = L_L + L_R$ naturally decomposes into
left-multiplication $L_L : X \mapsto AX$ and
right-multiplication $L_R : X \mapsto XA^\HT$.
Furthermore, $L_L$ and $L_R$ commute:
\begin{equation*}
  L_L \circ L_R = L_R \circ L_L = X \mapsto AXA^\HT
\end{equation*}
All the operators above are linear and act on $\Rnn$.
Apply the \ac{ADI}~\eqref{eq:adi:general2step} as a two-step method:
\begin{equation}
  \left\{
  \begin{aligned}
    (A + \alpha_k I_n) X_{k+\frac{1}{2}} &= -W - X_k (A^\HT - \alpha_k I_n) \\
    X_{k+\frac{1}{2}} (A^\HT + \beta I_n) &= -W - (A - \beta I_n) X_k
  \end{aligned}
  \right.
\end{equation}
The one-step formulation following \eqref{eq:adi:general1step} reads:
\begin{equation}
  (A + \alpha_k I_n)
  X_{k+1}
  (A + \conj{\beta_k} I_n)^\HT
  =
  (A - \beta_k I_n)
  X_k
  (A - \conj{\alpha_k} I_n)^\HT
  - (\alpha_k + \beta_k)
  W
\end{equation}
Choosing $\beta_k := \conj{\alpha_k}$
the above becomes Hermitean:
\begin{equation}
\label{eq:adi:ale1step}
  (A + \alpha_k I_n)
  X_{k+1}
  (A + \alpha_k I_n)^\HT
  =
  (A - \conj{\alpha_k} I_n)
  X_k
  (A - \conj{\alpha_k} I_n)^\HT
  - 2\Re(\alpha_k)
  W
\end{equation}
Define
$A^+_k := A + \alpha_k I_n$ and
$A^-_k := A - \conj\alpha_k I_n$.
For $X_k = L_k D_k L_k^\T$ and $W = GSG^\T$,
$G\in\R^{n\times n_G}$,
$S\in\R^{n_G\times n_G}$,
the \ac{ADI} can be stated in \ac{LRSIF} directly.
Using the update formulas of \autoref{sec:lr:update},
the iterates are build by adding successively adding column blocks of size $n_G$:
\begin{align*}
  L_{k+1} &= \begin{bmatrix}
    (A^+_k)^{-1} A^-_k L_k &
    (A^+_k)^{-1} G
  \end{bmatrix} \\
  D_{k+1} &= \begin{bmatrix}
    D_k \\
    & -2\Re(\alpha_k) S
  \end{bmatrix}
\end{align*}
Using an initial value $X^0=0$,
the $k$-th iterate thus has dimensions $L_k \in\R^{n\times kn_G}$ and $D_k \in\R^{kn_G\times kn_G}$.
As a consequence, computing the first block component $(A_k^+)^{-1} A_k^- L_k$ becomes increasingly expensive.
This can be remedied by a reordering according to \eqref{eq:adi:recursion},
\begin{equation}
\left\{
\begin{aligned}
  X_{k+1} &= X_k + V_k
  \\
  \Re(\alpha_k)^{-1}
  (A_k^+) V_k (A_k^+)^\HT
  &=
  \Re(\alpha_{k-1})^{-1}
  (A_{k-1}^-) V_{k-1} (A_{k-1}^-)^\HT
\end{aligned}
\right.
\end{equation}
with the corresponding low-rank formulation
\begin{equation*}
\left\{
\begin{aligned}
  L_{k+1} &= \begin{bmatrix}
    L_k &
    (A_k^+)^{-1} (A_{k-1}^-) \tilde V_k
  \end{bmatrix}
  \\
  D_{k+1} &= \begin{bmatrix}
    D_k \\
    & -2\Re(\alpha_k) S
  \end{bmatrix}
  \\
  \tilde V_k &= (A_k^+)^{-1} (A_{k-1}^-) \tilde V_{k-1}
  \\
  \tilde V_0 &= (A_0^+)^{-1} G
\end{aligned}
\right.
\end{equation*}
where $\tilde V_k \in\R^{n\times n_G}$ has constant dimensions.

\todo[inline]{Column Compression}

\begin{remark}
  Let $\alpha\in\C\setminus\R$ and $A_+ := A+\alpha I_n \in\Cnn$.
  As a mapping of matrices,
  $L_R + \alpha I_{n^2} : \Cnn \to \Cnn$ maps $U$ onto
  $
    U A^\HT + \alpha U =
    U(A+\conj{\alpha} I_n)^\HT \neq
    U A_+^\HT
  $.
  Therefore, the notation is slightly more involved than in the previous section.
  For more details, refer to \autoref{sec:vectorization}.
\end{remark}

\begin{remark}
  \citeauthor{Lang2017}~\cite{Lang2017} formulates the \ac{ALE}~\eqref{eq:adi:ale} in terms of the
  right-multiplication $\tilde L_R :X \mapsto XA^\T$, $\tilde L_R \neq L_R$.
  The ADI~\cite[Equation~(2.23)]{Lang2017} uses a somewhat inconsistent second step.
  This leads to the impression that the ADI is a one-parameter ADI having $\tilde\beta_k = \tilde\alpha_k$
  instead of being a true two-parameter ADI using $\beta_k = \conj{\alpha_k}$.
  The one-step formulation \cite[Equation~(2.24)]{Lang2017} is identical to \eqref{eq:adi:ale1step}.
\end{remark}

\todo[inline]{%
Can $\rho(M) \leq \norm{M}$ (for any operator norm) be used to further simplify \cite[Algorithm~2.2, line~2]{Lang2017},
\ie can $\norm{}_2$ be replaced by \eg $\norm{}_F$?
As we are dealing with low-rank formulations of known rank $r$,
given that we ensure full rank of the factors,
we can \enquote{safely} exploit $\norm{A}_2 \leq \norm{A}_F \leq \sqrt{r}\norm{A}_2$.
}

\section{Parameter Selection}
\label{sec:adi:parameters}

After deriving the structure of the algorithm for \ac{ALE},
and the need of $\beta_k := \conj{\alpha_k}$ for a low-rank formulation,
we have to determine suitable parameters $\Set{\alpha_k : k\in\N}$.
Classical choices of pre-computed values include
optimal Wachspress shifts \cite{Wachspress1992,Wachspress2013} and
heuristic Penzl shifts \cite{Penzl1999}.
We resort to self-generating shifts described in \cite[Section~5.3]{Kuerschner2016}.

\section{Alternative Lyapunov Solvers}
Krylov subspace,
projection-based,
hybrid

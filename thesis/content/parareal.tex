\chapter{Parareal Method}
\label{sec:pr}

The parareal method is a parallel-in-time integration scheme to solve \acp{ODE}.
It goes back to \cite{Lions2001},
though the most common form also used in this thesis first appeared in \cite{Baffico2002}.
In this chapter we'll first describe the general properties of the parareal method,
and derive it as a Newton method following \cite{Gander2007}.
Lastly, in \autoref{sec:pr:DRE} we'll adapt the method for \acp{LRSIF}.

\section{Overview}
\label{sec:pr:properties}

Consider the autonomous \ac{IVP}
\begin{equation}
  \label{eq:IVP}
  \left\{
  \begin{aligned}
    \dot u &= f \circ u \\
    u(t_0) &= u_0
  \end{aligned}
  \right.
\end{equation}
over the time span $[t_0, t_f]$.
Fix a discretization $t_0 < t_1 < \ldots < t_N = t_f$,
let $F$ denote a fine/high-precision solver of the \ac{IVP} above,
$F(t, t_0, u_0) \approx u(t)$,
and $G$ a coarse/lower-precision one.
Then the parareal method to compute $U_n \approx u(t_n)$ reads
\begin{equation}
  \label{eq:pr:method}
  \left\{
  \begin{aligned}
    U^0_{n+1} &:= G(U^0_n) \\
    U^{k+1}_{n+1} &:= G(U^{k+1}_n) + F(U^k_n) - G(U^k_n)
  \end{aligned}
  \right.
\end{equation}
where $U^0_0 := u_0$ and $G(U_n) := G(t_{n+1}, t_n, U_n)$,
$F(U_n) := F(t_{n+1}, t_n, U_n)$
integrate from~$t_n$ to~$t_{n+1}$.

The general data dependencies between the parareal iterates~$U_n^k$
based on \eqref{eq:pr:method} are shown in \autoref{fig:pr:DAG}.
Notice how in general each coarse solution $G(\optional{})$ is needed twice.
Therefore, a reasonable scheduling pattern of the directed acyclic graph of \autoref{fig:pr:DAG}
that exploits good data locality is a pipeline of stages $n$ which compute
\begin{equation}
\label{eq:pr:stage}
  \begin{tikzpicture}[
    baseline=(Un1k1.base),
    text height=1.5ex,
    text depth=0.25ex,
    >=Stealth,
    shorten <=0.5ex,
  ]
    \graph[grow right sep=0] {
      ""/"\ldots," -!- Gnk1/"$G(U_{n-1}^{k})$," -!- Un1k1/"$U_{n}^{k}$," -!- Fnk/"$F(U_{n-1}^{k})$," -!- ""/"\ldots"
    };
    \draw[<-] (Gnk1.south) -- +(-120:3ex);
    \draw[->] (Un1k1.north) -- +(60:3ex);
  \end{tikzpicture}
\end{equation}
where the arrows denote data dependencies or transmissions from and to the adjacent stages.
Note how the stages correspond to the columns of \autoref{fig:pr:DAG}.
Each stage~$n$ holds~$G(U_{n-1}^{k-1})$ and~$F(U_{n-1}^{k-1})$ in memory,
receives~$U_{n-1}^k$, and sends~$U_n^k$.
For this thesis, each stage~$n$ is mapped onto a fixed process.
However, due to \autoref{thm:pr:conv} some processes will finish earlier than others,
which leaves potential to better utilize given hardware.
More on this in \autoref{sec:results}.

\begin{figure}[t]
  \centering
  \input{figures/fig_parareal_dag2.tex}
  \caption{Dependencies between parareal values $U^{k}_{n}$}
  \label{fig:pr:DAG}
\end{figure}

\begin{proposition}[Diagonal Convergence]
\label{thm:pr:conv}
  Stage $n$ of the parareal method converges after at most $n$ iterations:
  \begin{equation*}
    U^k_n = U^n_n = F(U^{n-1}_{n-1})
    \quad
    \forall k \geq n
  \end{equation*}
\end{proposition}
\begin{proof}
  Due to $U^k_0 = u_0$ for all $k$,
  it follows by induction $\forall k \geq n$:
  \begin{equation*}
    \begin{array}{r@{}l@{}l@{}l}
      U^{k+1}_{n+1}
      :={} & F(U^k_n) &{}+ G(U^{k+1}_n) &{}- G(U^k_n) \\
      ={}  & F(U^n_n) &{}+ G(U^n_n)     &{}- G(U^n_n)
      = F(U^n_n)
    \end{array}
  \end{equation*}
\end{proof}

The early iterates $U_n^k$ for $k=n$ thus have fewer dependencies between them,
\cf \autoref{fig:pr:DAG:early}.
Furthermore, the same reasoning as in the proof of \autoref{thm:pr:conv} can
be applied if stage $n$ converges after refinement $k < n$,
\ie $U^{k+1}_n \approx U^k_n$,
causing stage $n+1$ to converge the iteration thereafter:
\begin{equation}
  \begin{array}{r@{}l@{}l@{}l}
    U^{k+1}_{n+1}
    :={}      & F(U^k_n) &{}+ G(U^{k+1}_n) &{}- G(U^k_n) \\
    \approx{} & F(U^k_n) &{}+ G(U^k_n)     &{}- G(U^k_n)
    = F(U^k_n)
  \end{array}
\end{equation}
Therefore,
assuming $G$ is well conditioned,
there is no need to compute $G(U^{k+1}_n)$.

\begin{figure}[t]
  \centering
  \input{figures/fig_parareal_dag1.tex}
  \caption{Dependencies between parareal values $U^0_0, \ldots, U_2^2$}
  \label{fig:pr:DAG:early}
\end{figure}

Following \autoref{thm:pr:conv},
the parareal method converges after $k=N$ iterations at the latest,
at which point the parareal solutions equal the fine solution,
$U_n^N = F(U_{n-1}^N)$ for $1\leq n \leq N$.
In practice, however, it may converge much earlier than that,
\ie~$k\ll N$,
as the method can be seen \eg as a Newton method.
This will be covered in the next section.
We end this section with an illustrative example.

\begin{example}[Linear \acs*{ODE}]
\label{example:parareal}
  Let $\phi := \frac{1+\sqrt{5}}{2}$ denote the golden ratio and
  let $\kappa := \frac{2}{\pi} \log \phi$.
  Consider the \ac{IVP}
  \begin{equation}
    \label{eq:pr:linear}
    \left\{
    \begin{aligned}
      \dot u &= \begin{bmatrix}
        \kappa & -1 \\
        1 & \kappa
      \end{bmatrix} u \\
      u(0) &= \begin{bmatrix}
        -1 \\ -\kappa
      \end{bmatrix}
    \end{aligned}
    \right.
  \end{equation}
  for $t\in[0,2\pi]$.
  Let $N := 5$, $\tau := 2\pi/N$, and $t_n := n\tau$ for $n\in\Set{0,\ldots,N}$.
  Further, let $F$ and $G$ be implicit Euler schemes using step sizes $\tau/100$ and $\tau$, respectively.
  If the fine solver internally uses a finer mesh,
  by \autoref{thm:pr:conv},
  the global solution $x$ may be retrieved at that fine resolution $\tau/100$.

  \begin{figure}[tb]
    \includegraphics[width=\textwidth]{figures/fig_parareal_example.pdf}
    \caption[Parareal method applied to a linear ODE]{%
      First couple of refinements $k$ of the
      parareal method applied to the linear problem \eqref{eq:pr:linear}.
      The stages $n$ are decoded by color,
      dots denoting $U^k_n$ and lines $F(\optional{}, t_{n-1}, U^k_{n-1})$.
      The gray curve represents the exact solution.
    }
    \label{fig:pr:linear}
  \end{figure}

  \autoref{fig:pr:linear} shows the first couple of iterations of the parareal method applied the problem above.
  For a fixed refinement $k$,
  the values $U^k_n \approx u(t_n)$ (represented by dots) are computed in sequence,
  \cf the horizontal traces in \autoref{fig:pr:DAG}.
  After that, the fine solutions $F(U_n^k)$ (represented by lines) can then be computed in parallel.
  The parareal method only ensures convergence of $U_n^k$.
  Therefore, before convergence is reached,
  the high-resolution trajectories are discontinues at $t_n$.
  Furthermore, note that the overall accuracy of the parareal solution is limited by the accuracy of $F$,
  which explains the remaining gap between the colored and the gray line.
\end{example}

\section{Parareal as a Multiple Shooting Method}
\label{sec:pr:newton}

Consider the autonomous \ac{IVP}
\begin{equation}
  \tag{\ref*{eq:IVP}}
  \left\{
  \begin{aligned}
    \dot u &= f \circ u \\
    u(t_0) &= u_0
  \end{aligned}
  \right.
\end{equation}
Note that the solution $u : [t_0,t_N] \to V $ depends on the initial value $U_0$.
For a discretization $t_0 < t_1 < \ldots < t_N$
define the local problems
\begin{equation}
  \left\{
  \begin{aligned}
    \dot u_i &= f \circ u_i \\
    u_i(t_i) &= U_i
  \end{aligned}
  \right.
\end{equation}
for $i \in \Set{0,\ldots,N-1}$
and determine suitable initial values $U_i$.
Watch the slight abuse of notation in $u_0$
for the initial value $u_0 \in V$
and the solution $u_0 : [t_0,t_1] \to V$ to the first local problem.
The local solutions $u_i : [t_i,t_{i+1}] \to V$ represent a global solution to \eqref{eq:IVP} if
\begin{equation}
  F(U_0, \ldots, U_{N-1}) :=
  \left(
  \begin{aligned}
    U_0 &- u_0 \\
    U_1 &- u_0(t_1) \\
    &\vdotswithin{-} \\
    U_{N-1} &- u_{N-2}(t_{N-1})
  \end{aligned}
  \right)
  \overset{!}{=} 0.
\end{equation}
Let $U := (U_0, \ldots, U_{N-1})$ and $\mathcal J_F(U)$ denote the Jacobian of $F$ at $U$.
Then, Newton's method reads
\begin{equation}
  U^{k+1} = U^k - \mathop{\mathcal J_F(U^k)^{-1}} F(U^k)
\end{equation}
or, equivalently, avoiding the inversion of the Jacobian,
\begin{equation}
  \begin{pmatrix}
    I \\
    -\pdiff{U_0}{u_0}(t_1, U_0) & I \\
    & \ddots & \ddots \\
    && -\pdiff{U_{N-2}}{u_{N-2}}(t_{N-1}, U_{N-2}) & I
  \end{pmatrix}
  (U^{k+1} - U^k) = -F(U^k)
\end{equation}
Writing the above in separate equations yields
\begin{equation}
  \left\{
  \begin{aligned}
    U^{k+1}_0 &= u_0 \\
    U^{k+1}_i &= u_i(t_{i+1}, U^k_n) + \pdiff{U^k_i}{u_i}(t_{i+1}, U^k_i) (U^{k+1}_i - U^k_i).
  \end{aligned}
  \right.
\end{equation}
Lastly, choosing a fine propagator $F(U^k_i) := F(t_{i+1}; t_i, U^k_i) \approx u_i(t_{i+1}, U^k_i)$,
a coarse propagator $G$ according to
\begin{equation}
  \label{eq:pr:G}
  G(U^{k+1}_i) - G(U^k_i)
  \approx
  \pdiff{U^k_i}{u_i}(t_{i+1}, U^k_i) (U^{k+1}_i - U^k_i)
\end{equation}
as well as initial values $ U^0_{i+1} = G(U^0_i) $
reveals the parareal method of \cite{Baffico2002}:
\begin{equation}
  \left\{
  \begin{aligned}
    U^0_{i+1} &= G(U^0_i),
    \quad
    U^0_0 = u_0 \\
    U^{k+1}_{i+1} &= F(U^k_i) + G(U^{k+1}_i) - G(U^k_i)
  \end{aligned}
  \right.
\end{equation}
A Taylor expansion of $G(U^{k+1}_i) \approx u_i(t_{i+1}, U^{k+1}_i)$ around $G(U^k_i)$ justifies \eqref{eq:pr:G}.

\section{Application to \texorpdfstring{\act{LRSIF}s}{LRSIFs}}
\label{sec:pr:DRE}

Let the \acp{LRSIF}
\begin{equation}
\begin{aligned}
  G(X^{k+1}_i) &= \mathop{L_{G,k+1}} \mathop{D_{G,k+1}} \mathop{L_{G,k+1}^\T} \\
  F(X^k_i)     &= \mathop{L_{F,k}}   \mathop{D_{F,k}}   \mathop{L_{F,k}^\T} \\
  G(X^k_i)     &= \mathop{L_{G,k}}   \mathop{D_{G,k}}   \mathop{L_{G,k}^\T}
\end{aligned}
\end{equation}
be given.
Using the low-rank update formulas described in \autoref{sec:lowrank},
the parareal update of \eqref{eq:pr:method} reads
\begin{equation}
  \hat L \hat D \hat L^\T :=
  \begin{pmatrix}
    L_{G,k+1} &
    L_{F,k} &
    L_{G,k}
  \end{pmatrix}
  \begin{pmatrix}
    D_{G,k+1} \\
    & D_{F,k} \\
    && -D_{G,k}
  \end{pmatrix}
  \begin{pmatrix}
    L_{G,k+1}^\T \\
    L_{F,k}^\T \\
    L_{G,k}^\T
  \end{pmatrix}
  .
\end{equation}
Note the downdate in the last block component.
The actual low-rank representation of $X^{k+1}_{i+1}$ is set to a column compression of $\hat L \hat D \hat L^\T$
using \autoref{alg:lowrank:compression}.

%\section{Alternative Parallel-in-Time Solvers}

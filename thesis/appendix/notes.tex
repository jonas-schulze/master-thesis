\chapter{Notes}

\begin{itemize}
  \item
    Does \Ros{2} work better if we don't condense the right-hand side of the second stage?
    That is, does
    \begin{equation}
      \tau^2 K_1 BB^\T K_1 + \big( 2-\tfrac{1}{\gamma} \big) K_1
      = \begin{bmatrix}
        T_1 & T_1
      \end{bmatrix}
      \begin{bmatrix}
        \tau^2 M_1 T_1^\T BB^\T T_1 M_1 \\
        &\big( 2-\tfrac{1}{\gamma} \big) M_1
      \end{bmatrix}
      \begin{bmatrix}
        T_1^\T \\
        T_1^\T
      \end{bmatrix}
    \end{equation}
    work better?
    Maybe it suffers from cancellation.
\end{itemize}

\begin{align}
  E\dot{X} &= AX + BU \\
  Y &= CX + DU \\
  X(t_0) &= X_0
\end{align}

\section{Linear Systems and Related Matrix Equations}

refer to \cite{Simoncini2016}

A \emph{generalized state-space system}
\begin{equation}
\label{eq:basics:system}
\left\{
\begin{aligned}
  E \dot x &= Ax + Bu\\
  y &= Cx
\end{aligned}
\right.
\end{equation}
describes the dynamics of the \emph{state} $x(t) \in\R^n$, $t\in [t_0, t_f]$,
under the influence of the \emph{input} or \emph{control} $u(t)\in\R^m$,
as observed by the \emph{output} $y(t) \in\R^q$.
The system consists of the so-called
\emph{mass matrix} $E(t) \in\Rnn$,
\emph{state-space matrix} $A(t) \in\Rnn$,
\emph{input map} $B(t) \in\R^{n\times m}$, and
\emph{output map} $C(t) \in\R^{q\times n}$,
where typically $m,q \ll n$.
If these \emph{system matrices} are constant over the whole time span $t \in [t_0, t_f]$,
\eqref{eq:basics:system} is called a \ac{LTI} system, and \ac{LTV} otherwise.
The first equation of \eqref{eq:basics:system} is called \emph{state equation},
the other \emph{ouput equation}.
If $E=I$ the system is said to be in \emph{standard state-space form}.

From system \eqref{eq:basics:system} one may derive a \ac{DRE}, \ac{ARE}, \ac{ALE}, \etc

\section{Equation Types}
\subsection{Standard}

Differential and algebraic Riccati equations:
\begin{align}
  \dot{X} &= A^\T X + XA - XSX + W \\
  0 &= A^\T X + XA - XSX + W \\
\intertext{Differential and algebraic Lyapunov equations:}
  \dot{X} &= A^\T X + XA + W \\
  0 &= A^\T X + XA + W
\end{align}

\subsection{Generalized}

Substituting $X\gets E^\T XE$ leads to generalized equations.
Generalized differential and algebraic Riccati equations (assuming $\dot E = 0$):
\begin{align}
  E^\T \dot{X}E &= A^\T XE + E^\T XA - E^\T XSXE + W \\
  0 &= A^\T XE + E^\T XA - E^\T XSXE + W \\
\intertext{Generalized differential and algebraic Lyapunov equations:}
  E^\T \dot{X}E &= A^\T XE + E^\T XA + W \\
  0 &= A^\T XE + E^\T XA + W \\
\end{align}

\cite{Lang2017}

\section{Low-Rank Factorizations}

\begin{proposition}[Triangular Low-Rank Factorizations]
  Let $X=ZZ^\T$ or $X=ZDZ^\T$ for some $Z\in\R^{n\times r}$ and $D\in\R^{r\times r}$,
  $r \leq n$,
  then there exist a lower triangular $\hat{Z}$
  and some $\hat D$ of the same size satisfying
  $X=\hat Z \hat Z^\T$
  or $X=\hat Z \hat D \hat Z^\T$.
\end{proposition}
\begin{proof}
  We are looking for a transformation $H\in\R^{r\times r}$ such that $\hat Z := ZH$ is triangular and $HH^\T = I_r$.
  Then $\hat Z := ZH$ and $\hat D := H^\T DH$ have the desired properties.

  \todo{How to handle row permutations?}
  Without loss of generality, let the first $r$ rows of $Z$ have full rank.
  Let $z_1\in\R^r$ denote the first row of $Z$.
  Let $H_r\in\R^{r\times r}$ be the \Householder transformation mapping
  $z_1$ onto $\norm{z_1}e_1 = (\norm{z_1},0,\ldots) \in \R^r$, \ie
  \begin{equation}
    H_r := I_r - \frac{2}{v^\T v} vv^T
  \end{equation}
  for $v := z_1 - \norm{z_1}e_1$.
  Note that $H_r H_r^\T = I_r$ and $H_r = H_r^\T$.
  As $Z$ has full rank, $v \neq 0$ and $H_r$ is well defined.
  Indeed,
  \begin{equation*}
    H_r z_1
    = z_1 - \frac{2v^\T z_1}{v^\T v} \cdot v
    = z_1 - 1 \cdot v
    = \norm{z_1}e_1
  \end{equation*}
  such that
  \begin{equation}
    Z H_r = (H_r Z^\T) = \begin{pmatrix}
      \norm{z_1} & 0 \\
      Z_{12} & Z_{22}
    \end{pmatrix}
  \end{equation}
  for some $Z_{12} \in \R^{n-1 \times 1}$ and $Z_{22} \in \R^{n-1 \times r-1}$.

  Similarly, let $z_2\in\R^{r-1}$ denote the first row of $Z_{22}$ and
  let $H_{r-1} \in \R^{r-1 \times r-1}$ be the \Householder matrix mapping
  $z_2$ onto $\norm{z_2}e_1 \in\R^{r-1}$.
  This iteratively defines the matrix
  \begin{equation}
    H :=
    H_r \cdot
    \begin{pmatrix}
      I_1 \\ & H_{r-1}
    \end{pmatrix}
    \cdots
    \begin{pmatrix}
      I_{r-2} \\ & H_2
    \end{pmatrix}
    .
  \end{equation}

  Note that $H_1 = I_1$ or $H_1 = -I_1$,
  which therefore isn't needed for the transformation into a triangular matrix.
  Further, note that every factor in the product above is itself a \Householder transformation acting on $\R^r$,
  whose corresponding vector has its leading components filled with zeros.
\end{proof}

\section{Software}

\cite{DrWatson}
\cite{Makie}

\section{Hamilton-Jacobi Theory}

Consider the optimal control problem
(linear quadratic regulator)
\begin{equation}
  \everymath{\displaystyle}
  \begin{array}{cl}
    \min_u & \int_{t_0}^{t_f} \ell(t, x(t), u(t)) \dt + m(t_f, x(t_f)) \\
    \text{s.t.} & \dot{x} = f(t,x,u), \enspace x(t_0) = x_0
  \end{array}
  \label{eq:optimal-control}
\end{equation}
using \emph{state}~$x:\R\to\R^n$, system \emph{control}~$u:\R\to\R^m$,
and scalar cost functions~$\ell$ and $m$.
An optimal control $u^*$ solving \eqref{eq:optimal-control} may be defined by means of the
\emph{value function}~$V:\R\times\R^n\to\R$ satisfying the \emph{Hamilton-Jacobi} equation
\begin{equation}
  V_t(t,x) + H(t,x,u^h,V_x^\T(t,x)) = 0.
  \label{eq:hamilton-jacobi}
\end{equation}
The \emph{Hamiltonian} $H:\R\times\R^n\times\R^m\times\R^n\to\R$ is given by
\begin{equation}
  H(t,x,u,\lambda) = \ell(t,x,u) + \lambda^\T f(t,x,u)
\end{equation}
and $u^h = u^h(t,x,V_x^\T)$ denotes the $H$-minimizing control.
In the context of the linear dynamical system
\begin{equation}
  \begin{aligned}
    E\dot{x} &= Ax + Bu \\
    y &= Cx
  \end{aligned}
\end{equation}
together with the state transformation $z=Ex$ and particular choices for $\ell$ and $m$,
making the ansatz
\begin{equation}
  V(t,z) = z^\T Xz + w^\T z + v
\end{equation}
and solving for $X:\R\to\R^{n,n}$, $w:\R\to\R^n$, $v:\R\to\R$ will eventually
yield a \emph{generalized differential Riccati equation} in $X$ as well as
ordinary differential equations in $w$ and $v$.
\cite{Locatelli2011}

\begin{align}
  \ell(t, z, u) &= z^\T \tilde C^\T Q \tilde C z - 2z^\T \tilde C^\T Q \hat y + \hat y^\T Q \hat y + u^\T R u \\
  \ell(t, x, u) &= x^\T C^\T Q C x - 2x^\T C^\T Q \hat y + \hat y^\T Q \hat y + u^\T R u
\end{align}
\cite[(3.12)]{Lang2017}

\section{ADI}

Observe how the operator split of the \ac{ADI} defines a fully commuting splitting scheme for $V_k$,
\cf \eqref{eq:adi:si-lr-adi}
\begin{equation}
\left\{
\begin{aligned}
  (\alpha_k + \conj{\alpha_{k-1}})^{-1} A_k^+ V_k &= (\alpha_k + \conj{\alpha_{k-1}})^{-1} A_{k-1}^- V_{k-1} + 0 \\
  V_0 &= (A_0^+)^{-1} G
\end{aligned}
\right.
\end{equation}
which looks for the kernel of
\begin{align*}
  (\alpha_k + \conj{\alpha_{k-1}})^{-1} (A_k^+ - A_{k-1}^-)
  &= (\alpha_k + \conj{\alpha_{k-1}})^{-1} \big( (A + \alpha_k I_n) - (A - \conj{\alpha_{k-1}} I_n) \big) \\
  &= (\alpha_k + \conj{\alpha_{k-1}})^{-1} (\alpha_k + \conj{\alpha_{k-1}}) I_n
  = I_n
  ,
\end{align*}
\ie $V_k$ converges to $0$.
By \autoref{thm:adi:residual},
the residual is given by
\begin{align*}
  \hat R_{k+1}
  &= (A_k^+)^{-1} A_{k-1}^- \hat R_k \\
  &= (I_n - (\alpha_k + \conj{\alpha_{k-1}}) (A_k^+)^{-1} \hat R_k \\
  &= \hat R_k - (\alpha_k + \conj{\alpha_{k-1}}) (A_k^+)^{-1} \hat R_k \\
  &= \hat R_k + (\alpha_k + \conj{\alpha_{k-1}}) \hat V_k
\end{align*}
